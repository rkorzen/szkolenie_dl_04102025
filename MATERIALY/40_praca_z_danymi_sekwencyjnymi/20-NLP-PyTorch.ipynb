{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f811ab",
   "metadata": {},
   "source": [
    "# Mini-projekt NLP w PyTorch\n",
    "Ten notatnik pokazuje najważniejsze koncepcje związane z przetwarzaniem języka naturalnego (NLP) w PyTorch na bardzo małym, sztucznym zbiorze danych. Przechodzimy przez cały proces: od przygotowania tekstu i budowy słownika, przez kodowanie sekwencji, aż po trenowanie prostej sieci RNN i wykonywanie prognoz.\n",
    "\n",
    "Postaramy dokonać analizy sentymentu - czyli określić, czy dany tekst jest pozytywny czy negatywny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36be30",
   "metadata": {},
   "source": [
    "## 1. Importy i ustawienia\n",
    "Zaczynamy od zaimportowania PyTorcha i kilku pomocniczych bibliotek. Ustawiamy także ziarno losowe, aby wyniki były powtarzalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf10fe0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cd87e30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53331f",
   "metadata": {},
   "source": [
    "## 2. Mini zbiór danych\n",
    "Dla przejrzystości wykorzystamy niewielki, ręcznie przygotowany zbiór zdań oznaczonych etykietami `1` (pozytywne) lub `0` (negatywne). Dzięki temu możemy szybko podejrzeć cały zbiór i łatwiej zrozumieć każdy krok przygotowania danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a01cd963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> kocham ten film\n",
      "1 -> uwielbiam tę książkę\n",
      "1 -> ten kurs jest świetny\n",
      "1 -> co za wspaniały dzień\n",
      "1 -> jestem zachwycony\n",
      "0 -> nienawidzę tej gry\n",
      "0 -> to był stracony czas\n",
      "0 -> film był okropny\n",
      "0 -> jestem rozczarowany\n",
      "0 -> fatalna obsługa\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    (\"kocham ten film\", 1),\n",
    "    (\"uwielbiam tę książkę\", 1),\n",
    "    (\"ten kurs jest świetny\", 1),\n",
    "    (\"co za wspaniały dzień\", 1),\n",
    "    (\"jestem zachwycony\", 1),\n",
    "    (\"nienawidzę tej gry\", 0),\n",
    "    (\"to był stracony czas\", 0),\n",
    "    (\"film był okropny\", 0),\n",
    "    (\"jestem rozczarowany\", 0),\n",
    "    (\"fatalna obsługa\", 0)\n",
    "]\n",
    "\n",
    "for text, label in samples:\n",
    "    print(f\"{label} -> {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057040c",
   "metadata": {},
   "source": [
    "## 3. Budowa słownika (vocabulary)\n",
    "Modelom NLP trudno pracować bezpośrednio na surowych słowach. Potrzebujemy mapowania słów na liczby całkowite. Budujemy słownik zawierający:\n",
    "\n",
    "- `PAD` – specjalny token wypełniający (używany przy wyrównywaniu długości sekwencji),\n",
    "- `UNK` – token dla słów nieznanych (gdyby pojawiły się inne słowa w nowych zdaniach),\n",
    "- wszystkie słowa z naszego mini zbioru.\n",
    "\n",
    "Tokenizacja będzie bardzo prosta – rozdzielamy słowa po spacjach i zamieniamy na małe litery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d0858f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar słownika: 28\n",
      "{'<PAD>': 0, '<UNK>': 1, 'kocham': 2, 'ten': 3, 'film': 4, 'uwielbiam': 5, 'tę': 6, 'książkę': 7, 'kurs': 8, 'jest': 9, 'świetny': 10, 'co': 11, 'za': 12, 'wspaniały': 13, 'dzień': 14, 'jestem': 15, 'zachwycony': 16, 'nienawidzę': 17, 'tej': 18, 'gry': 19, 'to': 20, 'był': 21, 'stracony': 22, 'czas': 23, 'okropny': 24, 'rozczarowany': 25, 'fatalna': 26, 'obsługa': 27}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "counter = Counter()\n",
    "for text, _ in samples:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in counter:\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(\"Rozmiar słownika:\", len(vocab))\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0f002",
   "metadata": {},
   "source": [
    "## 4. Kodowanie i wyrównywanie sekwencji\n",
    "Każde zdanie zamieniamy na listę indeksów słów. Następnie wyrównujemy długość sekwencji do stałej wartości `MAX_LEN`, dopełniając krótsze zdania tokenem `<PAD>`. Dzięki temu możemy umieścić dane w jednej macierzy tensora.\n",
    "\n",
    "W razie napotkania nieznanego słowa skorzystamy z `UNK`, choć w tym przykładzie wszystkie słowa są znane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70563699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowa sekwencja: [2, 3, 4, 0, 0] = ['kocham', 'ten', 'film', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 5\n",
    "\n",
    "def encode(text, vocab, max_len):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "encoded_texts = [encode(text, vocab, MAX_LEN) for text, _ in samples]\n",
    "labels = [label for _, label in samples]\n",
    "\n",
    "print(\"Przykładowa sekwencja:\", encoded_texts[0], \"=\", [inverse_vocab[idx] for idx in encoded_texts[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925a1ba",
   "metadata": {},
   "source": [
    "## 5. Dataset i DataLoader\n",
    "Korzystamy ze standardowej struktury PyTorch: tworzymy klasę `Dataset`, która zwraca pary `(tensor_wejściowy, etykieta)`. Następnie pakujemy dane w `DataLoader`, aby łatwo iterować po mini-batchach podczas treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "177f2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar zbioru treningowego: 8\n",
      "Rozmiar zbioru testowego: 2\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded = torch.tensor(encoded_texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded[idx], self.labels[idx]\n",
    "\n",
    "full_dataset = SentimentDataset(encoded_texts, labels)\n",
    "\n",
    "# Prosty podział: 8 przykładów treningowych, 2 testowe\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [8, 2], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "print('Rozmiar zbioru treningowego:', len(train_dataset))\n",
    "print('Rozmiar zbioru testowego:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee68d1",
   "metadata": {},
   "source": [
    "## 6. Model RNN\n",
    "Model składa się z trzech elementów:\n",
    "1. `nn.Embedding` – zamienia indeksy słów na gęste wektory (embeddingi).\n",
    "2. `nn.RNN` – przetwarza sekwencję i zwraca stany ukryte.\n",
    "3. `nn.Linear` + `nn.Sigmoid` – mapuje ostatni stan ukryty na prawdopodobieństwo klasy `1`.\n",
    "\n",
    "Utrzymujemy model możliwie prosty, aby skupić się na przepływie danych i interpretacji wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b3fff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSentimentRNN(\n",
       "  (embedding): Embedding(28, 16, padding_idx=0)\n",
       "  (rnn): RNN(16, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleSentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # hidden ma kształt (num_layers, batch, hidden_size)\n",
    "        last_hidden = hidden[-1]\n",
    "        logits = self.fc(last_hidden)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze(1)\n",
    "\n",
    "model = SimpleSentimentRNN(vocab_size=len(vocab))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad69c63",
   "metadata": {},
   "source": [
    "## 7. Trening\n",
    "Używamy binarnej funkcji straty (`BCELoss`) oraz optymalizatora Adam. Ponieważ danych jest mało, wystarczy kilkanaście epok, aby model nauczył się prostych reguł. W każdej epoce wypisujemy stratę i dokładność na zbiorze treningowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c596045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoka 01 | Strata: 0.7180 | Dokładność: 0.38\n",
      "Epoka 02 | Strata: 0.5905 | Dokładność: 0.75\n",
      "Epoka 03 | Strata: 0.4511 | Dokładność: 1.00\n",
      "Epoka 04 | Strata: 0.2809 | Dokładność: 1.00\n",
      "Epoka 05 | Strata: 0.1425 | Dokładność: 1.00\n",
      "Epoka 06 | Strata: 0.0680 | Dokładność: 1.00\n",
      "Epoka 07 | Strata: 0.0377 | Dokładność: 1.00\n",
      "Epoka 08 | Strata: 0.0213 | Dokładność: 1.00\n",
      "Epoka 09 | Strata: 0.0129 | Dokładność: 1.00\n",
      "Epoka 10 | Strata: 0.0086 | Dokładność: 1.00\n",
      "Epoka 11 | Strata: 0.0060 | Dokładność: 1.00\n",
      "Epoka 12 | Strata: 0.0045 | Dokładność: 1.00\n",
      "Epoka 13 | Strata: 0.0034 | Dokładność: 1.00\n",
      "Epoka 14 | Strata: 0.0027 | Dokładność: 1.00\n",
      "Epoka 15 | Strata: 0.0022 | Dokładność: 1.00\n",
      "Epoka 16 | Strata: 0.0019 | Dokładność: 1.00\n",
      "Epoka 17 | Strata: 0.0016 | Dokładność: 1.00\n",
      "Epoka 18 | Strata: 0.0015 | Dokładność: 1.00\n",
      "Epoka 19 | Strata: 0.0013 | Dokładność: 1.00\n",
      "Epoka 20 | Strata: 0.0012 | Dokładność: 1.00\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total\n",
    "    acc = correct / total\n",
    "    print(f\"Epoka {epoch:02d} | Strata: {avg_loss:.4f} | Dokładność: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aeb23",
   "metadata": {},
   "source": [
    "## 8. Ewaluacja na zbiorze testowym\n",
    "Sprawdzamy, jak model radzi sobie na dwóch przykładach testowych, które nie brały udziału w treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90475e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność na teście: 0.50 (1/2)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_correct += (preds == targets).sum().item()\n",
    "        test_total += targets.size(0)\n",
    "\n",
    "print(f'Dokładność na teście: {test_correct / test_total:.2f} ({test_correct}/{test_total})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed40e44",
   "metadata": {},
   "source": [
    "## 9. Predykcje dla nowych zdań\n",
    "Przygotowujemy pomocniczą funkcję, która przyjmuje tekst, koduje go tak jak wcześniej, a następnie zwraca prawdopodobieństwo klasy pozytywnej. Dzięki temu możemy szybko sprawdzić, jak model reaguje na nowe wypowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16bb68e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'kocham tę grę' -> prawdopodobieństwo klasy pozytywnej: 1.00\n",
      "'co za fatalny dzień' -> prawdopodobieństwo klasy pozytywnej: 0.04\n",
      "'jestem bardzo zadowolony' -> prawdopodobieństwo klasy pozytywnej: 0.99\n",
      "'strasznie nudny film' -> prawdopodobieństwo klasy pozytywnej: 0.58\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    encoded = torch.tensor([encode(text, vocab, MAX_LEN)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        prob = model(encoded).item()\n",
    "    return prob\n",
    "\n",
    "examples = [\n",
    "    \"kocham tę grę\",\n",
    "    \"co za fatalny dzień\",\n",
    "    \"jestem bardzo zadowolony\",\n",
    "    \"strasznie nudny film\"\n",
    "]\n",
    "\n",
    "for sentence in examples:\n",
    "    prob = predict_sentiment(model, sentence)\n",
    "    print(f\"{sentence!r} -> prawdopodobieństwo klasy pozytywnej: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4445d",
   "metadata": {},
   "source": [
    "## 10. Podsumowanie\n",
    "W tym mini-projekcie omówiliśmy podstawowy przepływ pracy w NLP z użyciem PyTorcha:\n",
    "\n",
    "1. **Przygotowanie danych** – tokenizacja, budowa słownika, kodowanie i wyrównanie sekwencji.\n",
    "2. **Struktura danych w PyTorch** – `Dataset` i `DataLoader` ułatwiają iterację po mini-batchach.\n",
    "3. **Model** – `Embedding` + `RNN` + warstwa gęsta generująca prawdopodobieństwo klasy.\n",
    "4. **Trening i ewaluacja** – klasyczna pętla treningowa z funkcją straty `BCELoss` oraz pomiarem dokładności.\n",
    "5. **Predykcje** – prosty interfejs do oceniania nowych zdań.\n",
    "\n",
    "Chociaż przykład jest niewielki, przedstawia wszystkie najważniejsze elementy pipeline'u NLP. Przy większych projektach wystarczy wymienić zbiór danych, rozszerzyć preprocessing (np. usuwanie znaków, stemming), zastosować bogatszą architekturę (GRU/LSTM/Transformer) i zwiększyć rozmiar embeddingów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338467bb",
   "metadata": {},
   "source": [
    "## 11. Rozszerzenie: proste techniki poprawy modelu\n",
    "Na koniec prezentujemy dwie techniki, które często poprawiają wyniki modeli NLP, nawet w bardzo prostych scenariuszach:\n",
    "\n",
    "- **Augmentacja danych** – tworzymy dodatkowe przykłady na podstawie istniejących, np. zamieniając niektóre słowa na synonimy.\n",
    "- **Walidacja krzyżowa** – uczymy model na kilku podziałach danych i uśredniamy wyniki, dzięki czemu ocena jest bardziej stabilna na małych zbiorach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb46979",
   "metadata": {},
   "source": [
    "### 11.1 Augmentacja danych (synonimy/perturbacje)\n",
    "W NLP augmentacja danych polega na sztucznym rozszerzaniu zbioru treningowego. W tym przykładzie losowo wybierzemy zdanie pozytywne i zamienimy w nim jedno słowo na prosty synonim (utrzymujemy minimalizm kodu). Celem jest pokazanie koncepcji – na poważne projekty warto używać bardziej rozbudowanych narzędzi (np. WordNet, bibliotek typu `nlpaug`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec523e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalne i zaugmentowane przykłady:\n",
      "'kocham ten film' -> 'uwielbiam ten film'\n",
      "'uwielbiam tę książkę' -> 'co za świetny dzień'\n",
      "'ten kurs jest świetny' -> 'nie cierpię tej gry'\n",
      "'co za wspaniały dzień' -> 'okropna obsługa'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "synonym_dict = {\n",
    "    \"kocham\": [\"uwielbiam\", \"bardzo lubię\"],\n",
    "    \"wspaniały\": [\"świetny\", \"znakomity\"],\n",
    "    \"fatalna\": [\"okropna\", \"beznadziejna\"],\n",
    "    \"nienawidzę\": [\"nie cierpię\"],\n",
    "}\n",
    "\n",
    "augmented_samples = []\n",
    "for text, label in samples:\n",
    "    tokens = tokenize(text)\n",
    "    new_tokens = tokens.copy()\n",
    "    candidate_positions = [i for i, tok in enumerate(tokens) if tok in synonym_dict]\n",
    "    if candidate_positions:\n",
    "        pos = random.choice(candidate_positions)\n",
    "        new_tokens[pos] = random.choice(synonym_dict[new_tokens[pos]])\n",
    "        new_text = \" \".join(new_tokens)\n",
    "        augmented_samples.append((new_text, label))\n",
    "\n",
    "print(\"Oryginalne i zaugmentowane przykłady:\")\n",
    "for original, augmented in zip(samples, augmented_samples):\n",
    "    print(f\"{original[0]!r} -> {augmented[0]!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1dc18",
   "metadata": {},
   "source": [
    "Teraz połączymy pierwotny zbiór z zaugmentowanym i ponownie zakodujemy sekwencje. W praktyce należałoby przebudować słownik, aby uwzględniał nowe słowa. Tutaj dla prostoty wykorzystamy istniejący słownik i potraktujemy ewentualne nowe słowa jako `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e0a7606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba przykładów po augmentacji: 14\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset = samples + augmented_samples\n",
    "\n",
    "encoded_augmented = [encode(text, vocab, MAX_LEN) for text, _ in augmented_dataset]\n",
    "labels_augmented = [label for _, label in augmented_dataset]\n",
    "\n",
    "print('Liczba przykładów po augmentacji:', len(encoded_augmented))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cf873",
   "metadata": {},
   "source": [
    "### 11.2 Walidacja krzyżowa na małym zbiorze\n",
    "Na małych zbiorach zwykły podział na trening/test może dawać niestabilne wyniki. **Walidacja krzyżowa (k-fold cross-validation)** dzieli zbiór na `K` fragmentów, z których każdy pełni rolę walidacyjną raz, a pozostałe służą do treningu. Uśredniamy wyniki, aby zredukować wpływ losowego podziału."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfa0b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | Dokładność walidacyjna: 0.80\n",
      "Fold 2 | Dokładność walidacyjna: 0.40\n",
      "Fold 3 | Dokładność walidacyjna: 1.00\n",
      "Średnia dokładność z walidacji krzyżowej: 0.7333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/6wst9f1n1g96lc__ld426szh0000gn/T/ipykernel_39040/567183548.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.encoded = torch.tensor(encoded_texts, dtype=torch.long)\n",
      "/var/folders/ff/6wst9f1n1g96lc__ld426szh0000gn/T/ipykernel_39040/567183548.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded_augmented, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels_augmented, dtype=torch.float32)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(encoded_tensor)):\n",
    "    train_subset = SentimentDataset(encoded_tensor[train_idx], labels_tensor[train_idx])\n",
    "    val_subset = SentimentDataset(encoded_tensor[val_idx], labels_tensor[val_idx])\n",
    "\n",
    "    train_loader_cv = DataLoader(train_subset, batch_size=4, shuffle=True)\n",
    "    val_loader_cv = DataLoader(val_subset, batch_size=4)\n",
    "\n",
    "    cv_model = SimpleSentimentRNN(len(vocab))\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(cv_model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        cv_model.train()\n",
    "        for inputs, targets in train_loader_cv:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cv_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cv_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_cv:\n",
    "            outputs = cv_model(inputs)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    fold_results.append(acc)\n",
    "    print(f'Fold {fold + 1} | Dokładność walidacyjna: {acc:.2f}')\n",
    "\n",
    "print('Średnia dokładność z walidacji krzyżowej:', np.mean(fold_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c947cd9",
   "metadata": {},
   "source": [
    "## 12. Co nam dają te techniki?\n",
    "- **Augmentacja** zwiększa różnorodność danych, dzięki czemu model może nauczyć się bardziej ogólnych wzorców (np. rozpoznawać synonimy). W realnych projektach często korzysta się z bardziej zaawansowanych metod – zamiany słów na synonimy, losowe usuwanie/ dodawanie słów, zamiany zdań na równoważne itp.\n",
    "- **Walidacja krzyżowa** dostarcza stabilniejszej oceny jakości modelu – zwłaszcza na małej liczbie przykładów. Uśredniony wynik lepiej odzwierciedla realną skuteczność niż pojedynczy podział na trening/test.\n",
    "\n",
    "Obie techniki są tanim sposobem na zwiększenie odporności modeli NLP na małych zbiorach i warto je mieć w zestawie narzędzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8a3c0-415d-4ddf-ac75-acb76ce4f2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
