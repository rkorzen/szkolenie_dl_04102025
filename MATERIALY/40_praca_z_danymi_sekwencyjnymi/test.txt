{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prognozowanie szeregów czasowych w PyTorch (RNN)\n",
        "\n",
        "W tym notatniku tworzymy prosty przykład prognozowania szeregów czasowych metodą uczenia nadzorowanego.\n",
        "\n",
        "- Generujemy syntetyczny szereg czasowy (sinus + trend + szum).\n",
        "- Tworzymy z niego próbki typu sekwencja->następna próbka (sequence-to-one).\n",
        "- Budujemy i trenujemy model RNN (`nn.RNN`) w PyTorch.\n",
        "- Oceniamy jakość prognoz i wizualizujemy wyniki.\n",
        "\n",
        "Uwaga: celowo używamy prostej warstwy `nn.RNN`, aby podkreślić podstawy pracy z danymi sekwencyjnymi. W praktyce często korzysta się z `LSTM` lub `GRU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importy\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ustawienia i deterministyczność\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generowanie i wizualizacja danych\n",
        "Tworzymy sztuczny szereg czasowy: sinus o wolno zmieniającej się częstotliwości + trend liniowy + szum gaussowski."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametry szeregu\n",
        "N = 1000             # liczba punktów\n",
        "trend = 0.002        # nachylenie trendu\n",
        "noise_std = 0.1      # odchylenie standardowe szumu\n",
        "\n",
        "t = np.arange(N)\n",
        "# Sinus z delikatną modulacją częstotliwości\n",
        "x = np.sin(2 * np.pi * t / 50.0 + 0.002*t)\n",
        "x += trend * t\n",
        "x += np.random.normal(0, noise_std, size=N)\n",
        "\n",
        "# Normalizacja (standaryzacja)\n",
        "mu, sigma = x.mean(), x.std()\n",
        "x_norm = (x - mu) / (sigma + 1e-8)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(t, x_norm, label='szereg znormalizowany')\n",
        "plt.title('Syntetyczny szereg czasowy (znormalizowany)')\n",
        "plt.xlabel('t')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "x_norm[:5], mu, sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tworzenie próbek sekwencyjnych (okna czasowe)\n",
        "Z sekwencji tworzymy przykłady (wejście, wyjście), gdzie wejściem jest okno o długości `SEQ_LEN`, a wyjściem kolejna próbka (one-step ahead)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEQ_LEN = 40\n",
        "\n",
        "def create_sequences(series: np.ndarray, seq_len: int):\n",
        "    X, y = [], []\n",
        "    for i in range(len(series) - seq_len):\n",
        "        X.append(series[i:i+seq_len])\n",
        "        y.append(series[i+seq_len])\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "X_all, y_all = create_sequences(x_norm, SEQ_LEN)\n",
        "X_all.shape, y_all.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Podział na zbiory treningowy i testowy + DataLoader\n",
        "Użyjemy ostatnich 20% danych jako test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_idx = int(0.8 * len(X_all))\n",
        "X_train, y_train = X_all[:split_idx], y_all[:split_idx]\n",
        "X_test, y_test = X_all[split_idx:], y_all[split_idx:]\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # RNN w PyTorch oczekuje wejścia o kształcie (seq_len, batch, input_size) podczas forward,\n",
        "        # ale my będziemy transponować w batchu w collate albo tu przekształcimy w torch.Tensor i zrobimy view w modelu.\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)  # (N, seq_len)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # (N,)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        # Dodajemy wymiar input_size=1 dla każdego kroku czasowego\n",
        "        return self.X[idx].unsqueeze(-1), self.y[idx]  # (seq_len, 1), ()\n",
        "\n",
        "train_ds = SeqDataset(X_train, y_train)\n",
        "test_ds = SeqDataset(X_test, y_test)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "len(train_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model RNN w PyTorch\n",
        "Użyjemy prostej architektury: `nn.RNN` -> `nn.Linear` (mapującej ostatni stan ukryty na prognozę)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNRegressor(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, nonlinearity='tanh'):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            nonlinearity=nonlinearity,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size) lub (seq_len, batch, input_size)\n",
        "        # My dostarczamy x jako (batch, seq_len, 1) -> zamienimy na (seq_len, batch, 1)\n",
        "        if x.dim() == 3 and x.shape[0] != x.shape[1] and x.shape[0] == x.shape[1]:\n",
        "            pass\n",
        "        # oczekiwany kształt przez nn.RNN (batch_first=False): (seq_len, batch, input_size)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch, input_size)\n",
        "        out_seq, h_n = self.rnn(x)\n",
        "        # używamy ostatniego stanu ukrytego (z ostatniego kroku): out_seq[-1] o kształcie (batch, hidden_size)\n",
        "        last_hidden = out_seq[-1]\n",
        "        out = self.fc(last_hidden)  # (batch, 1)\n",
        "        return out.squeeze(-1)      # (batch,)\n",
        "\n",
        "model = RNNRegressor(hidden_size=64).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Trening\n",
        "Trenujemy model w celu minimalizacji MSE pomiędzy prognozowanym a rzeczywistym następnym punktem szeregu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "EPOCHS = 40\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        # xb: (batch, seq_len, 1) po modyfikacji w Dataset\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # Dodatkowe upewnienie się, że ma wymiar batch-first na wejściu do naszego forward\n",
        "        xb = xb  # (batch, seq_len, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    epoch_loss /= len(train_ds)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Walidacja na teście (one-step)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vloss = 0.0\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            vloss += loss.item() * xb.size(0)\n",
        "        vloss /= len(test_ds)\n",
        "        val_losses.append(vloss)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f'Epoch {epoch:3d} | train MSE: {epoch_loss:.4f} | val MSE: {vloss:.4f}')\n",
        "\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.title('Przebieg funkcji straty (MSE)')\n",
        "plt.xlabel('Epoka')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ewaluacja: prognoza rekurencyjna na zbiorze testowym\n",
        "W prognozowaniu wielokrokowym często stosuje się podejście rekurencyjne:\n",
        "- Startujemy od ostatniego okna ze zbioru treningowego,\n",
        "- przewidujemy kolejny punkt,\n",
        "- dołączamy go do okna i usuwamy najstarszy element,\n",
        "- powtarzamy dla całej długości zbioru testowego.\n",
        "\n",
        "Poniżej wizualizujemy wynik na oryginalnej skali (odwracamy normalizację)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Odwracanie normalizacji\n",
        "def denorm(z):\n",
        "    return z * (sigma + 1e-8) + mu\n",
        "\n",
        "# Początkowe okno: ostatnie okno ze zbioru treningowego (w oryginalnej znormalizowanej skali)\n",
        "start_idx = split_idx  # indeks pierwszego punktu testowego w danych okienkowych\n",
        "init_window = X_all[start_idx-1]  # okno poprzedzające pierwszy punkt testowy\n",
        "\n",
        "# Rekurencyjna prognoza na długość testu\n",
        "model.eval()\n",
        "window = torch.tensor(init_window, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)  # (1, seq_len, 1)\n",
        "preds_test = []\n",
        "with torch.no_grad():\n",
        "    for _ in range(len(y_test)):\n",
        "        yhat = model(window).item()            # skala znormalizowana\n",
        "        preds_test.append(yhat)\n",
        "        # Przesuwamy okno: usuwamy najstarszy, dodajemy nową prognozę\n",
        "        next_step = torch.tensor([[yhat]], dtype=torch.float32, device=device)  # (1,1)\n",
        "        window = torch.cat([window[:, 1:, :], next_step.unsqueeze(1)], dim=1)\n",
        "\n",
        "# Przygotowanie do wizualizacji\n",
        "true_test_denorm = denorm(y_test)\n",
        "preds_test_denorm = denorm(np.array(preds_test))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(denorm(x_norm), label='prawdziwy szereg', alpha=0.6)\n",
        "# Zakres indeksów dla przewidywań testowych w oryginalnej osi czasu\n",
        "test_time_index = np.arange(split_idx + SEQ_LEN, split_idx + SEQ_LEN + len(preds_test_denorm))\n",
        "plt.plot(test_time_index, preds_test_denorm, label='prognoza (rekurencyjna RNN)')\n",
        "plt.axvline(split_idx + SEQ_LEN, color='k', linestyle='--', alpha=0.5, label='granica train/test')\n",
        "plt.title('Prognoza rekurencyjna na zbiorze testowym')\n",
        "plt.xlabel('t')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Prosta metryka MSE na teście (porównując do prawdy one-step)\n",
        "mse_recursive = np.mean((preds_test - y_test)**2)\n",
        "print('MSE (rekurencyjna prognoza vs. y_test, skala znormalizowana):', round(float(mse_recursive), 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. (Opcjonalnie) Zapis i odczyt wytrenowanego modelu\n",
        "Najczęściej zapisujemy same wagi (`state_dict`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = 'rnn_timeseries_state_dict.pt'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print('Zapisano wagi do:', save_path)\n",
        "\n",
        "# Załadowanie (przykład):\n",
        "loaded = RNNRegressor(hidden_size=64).to(device)\n",
        "loaded.load_state_dict(torch.load(save_path, map_location=device))\n",
        "loaded.eval()\n",
        "print('Model wczytano poprawnie.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Podsumowanie\n",
        "- Zbudowaliśmy prosty pipeline do prognozowania szeregów czasowych w PyTorch z użyciem `nn.RNN`.\n",
        "- Pokazaliśmy przygotowanie danych (okna czasowe), trening, walidację oraz prognozowanie rekurencyjne i wizualizację wyników.\n",
        "- W praktyce warto eksperymentować z długością okna, rozmiarem warstw, liczbą warstw oraz użyciem `LSTM`/`GRU`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}