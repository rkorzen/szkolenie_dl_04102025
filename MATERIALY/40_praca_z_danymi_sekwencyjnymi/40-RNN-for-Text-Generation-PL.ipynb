{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7de666",
   "metadata": {},
   "source": [
    "# RNN do generowania tekstu\n",
    "\n",
    "(To można spróbować na google Colab albo na jakieś mocniejszej maszynie)\n",
    "\n",
    "## Generowanie tekstu (zakodowane zmienne)\n",
    "\n",
    "Do tej pory potrafiliśmy generować wartości ciągłe, a teraz zobaczymy, jak uogólnić to podejście na sekwencje kategoryczne (np. słowa lub litery).\n",
    "\n",
    "## Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456c799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343056ea",
   "metadata": {},
   "source": [
    "## Wczytanie danych tekstowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332bca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07418ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1218a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba14a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb986e8",
   "metadata": {},
   "source": [
    "## Kodowanie całego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d0e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b9f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4f1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e116d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "# decoder.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fd4582",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1babf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5030985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0fb7abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "       42, 42, 42, 42, 42, 60, 49, 42, 42, 71, 38, 22, 62, 42, 25, 72, 28,\n",
       "       38, 13,  4, 10, 42, 41, 38, 13, 72, 10, 39, 38, 13,  4, 42, 79, 13,\n",
       "       42, 59, 13,  4, 28, 38, 13, 42, 28, 19, 41, 38, 13, 72,  4, 13, 33,\n",
       "       49, 42, 42, 61, 30, 72, 10, 42, 10, 30, 13, 38, 13,  7,  2, 42,  7,\n",
       "       13, 72, 39, 10,  2, 70,  4, 42, 38, 22,  4, 13, 42, 62, 28, 21, 30,\n",
       "       10, 42, 19, 13, 69, 13, 38, 42, 59, 28, 13, 33, 49, 42, 42, 36, 39,\n",
       "       10, 42, 72,  4, 42, 10, 30, 13, 42, 38, 28,  8, 13, 38, 42,  4, 30,\n",
       "       22, 39,  9, 59, 42,  7,  2, 42, 10, 28, 62, 13, 42, 59, 13, 41, 13,\n",
       "       72,  4, 13, 33, 49, 42, 42, 46, 28,  4, 42, 10, 13, 19, 59, 13, 38,\n",
       "       42, 30, 13, 28, 38, 42, 62, 28, 21, 30, 10, 42,  7, 13, 72, 38, 42,\n",
       "       30, 28,  4, 42, 62, 13, 62, 22, 38,  2, 14, 49, 42, 42, 36, 39, 10,\n",
       "       42, 10, 30, 22, 39, 42, 41, 22, 19, 10, 38, 72, 41, 10, 13, 59, 42,\n",
       "       10, 22, 42, 10, 30, 28, 19, 13, 42, 22, 79, 19, 42,  7, 38, 28, 21,\n",
       "       30, 10, 42, 13,  2, 13,  4, 33, 49, 42, 42, 71, 13, 13, 59, 70,  4,\n",
       "       10, 42, 10, 30,  2, 42,  9, 28, 21, 30, 10, 70,  4, 42, 25,  9, 72,\n",
       "       62, 13, 42, 79, 28, 10, 30, 42,  4, 13,  9, 25, 23,  4, 39,  7,  4,\n",
       "       10, 72, 19, 10, 28, 72,  9, 42, 25, 39, 13,  9, 33, 49, 42, 42, 18,\n",
       "       72, 80, 28, 19, 21, 42, 72, 42, 25, 72, 62, 28, 19, 13, 42, 79, 30,\n",
       "       13, 38, 13, 42, 72,  7, 39, 19, 59, 72, 19, 41, 13, 42,  9, 28, 13,\n",
       "        4, 33, 49, 42, 42, 61, 30,  2, 42,  4, 13,  9, 25, 42, 10, 30,  2,\n",
       "       42, 25, 22, 13, 33, 42, 10, 22, 42, 10, 30,  2, 42,  4, 79, 13, 13,\n",
       "       10, 42,  4, 13,  9, 25, 42, 10, 22, 22, 42, 41, 38, 39, 13,  9, 14,\n",
       "       49, 42, 42, 61, 30, 22, 39, 42, 10, 30, 72, 10, 42, 72, 38, 10, 42,\n",
       "       19, 22, 79, 42, 10, 30, 13, 42, 79, 22, 38,  9, 59, 70,  4, 42, 25,\n",
       "       38, 13,  4, 30, 42, 22, 38, 19, 72, 62, 13, 19, 10, 33, 49, 42, 42,\n",
       "       12, 19, 59, 42, 22, 19,  9,  2, 42, 30, 13, 38, 72,  9, 59, 42, 10,\n",
       "       22, 42, 10, 30, 13, 42, 21, 72, 39, 59,  2, 42,  4,  8, 38, 28, 19,\n",
       "       21, 33, 49, 42, 42, 53, 28, 10, 30, 28, 19, 42, 10, 30, 28, 19, 13,\n",
       "       42, 22, 79, 19, 42,  7, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a9c30",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Jak już wspominaliśmy, musimy zakodować dane w postaci one-hot, aby współgrały ze strukturą sieci. Jeśli któryś z kroków budzi wątpliwości, wróć na chwilę do notatek o NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0327832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # Reshape it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c151bdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743059e2",
   "metadata": {},
   "source": [
    "---\n",
    "# Tworzenie partii treningowych\n",
    "\n",
    "Musimy przygotować funkcję, która będzie generowała partie znaków oraz kolejny znak w sekwencji traktowany jako etykieta.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655ea672",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3218fd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd9904a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we wanted 5 batches\n",
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c45e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b3be2",
   "metadata": {},
   "source": [
    "### Przykład generowania partii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3beae9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d78476a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "       42, 42, 42])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac7fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "966fe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first batch\n",
    "x, y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b781775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c524d4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311345c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cdc353",
   "metadata": {},
   "source": [
    "## Sprawdzenie dostępności GPU\n",
    "\n",
    "Pamiętaj, że trenowanie na CPU potrwa znacznie dłużej!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01c1ef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59071588",
   "metadata": {},
   "source": [
    "# Tworzenie modelu LSTM\n",
    "\n",
    "**Uwaga! Pokażemy warianty dla użytkowników GPU i CPU. Trening na CPU trwa ZNACZNIE dłużej i może prowadzić do problemów z pamięcią RAM w zależności od sprzętu. W razie potrzeby rozważ użycie chmury (AWS, GCP, Azure) – pamiętaj, że to może wiązać się z kosztami.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3210e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389ea56",
   "metadata": {},
   "source": [
    "## Utworzenie instancji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eb8fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9235f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cae195",
   "metadata": {},
   "source": [
    "Spróbuj dobrać liczbę parametrów tak, by była zbliżona do liczby znaków w całym tekście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2c38e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6bb1732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d1ad5",
   "metadata": {},
   "source": [
    "### Optymalizator i funkcja straty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e044616",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9dd9d2",
   "metadata": {},
   "source": [
    "## Podział na dane treningowe i walidacyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f65d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef61246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b98dcba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544560"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5abc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeb45d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb3f7b",
   "metadata": {},
   "source": [
    "# Trenowanie sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8b048",
   "metadata": {},
   "source": [
    "## Parametry\n",
    "\n",
    "Eksperymentuj z poniższymi wartościami, aby zobaczyć, jak wpływają na wyniki!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "546c11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c95071",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da69841",
   "metadata": {},
   "source": [
    "---\n",
    "## Zapisywanie modelu\n",
    "\n",
    "[Dokumentacja PyTorch: zapisywanie i wczytywanie modeli](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72a017e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'example.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35b2921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95753a",
   "metadata": {},
   "source": [
    "## Wczytywanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a13a1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7799e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb5182",
   "metadata": {},
   "source": [
    "# Generowanie prognoz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0a1c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd1d76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e77392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0017fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The will true and breathed to me.\n",
      "    If thou wert better to the stare and send thee,\n",
      "    Which hath any trives and sound and stretged,\n",
      "    That have the better send of the constance,\n",
      "    That then that thou shaltst but that have seem surpet\n",
      "    And we had been the self-fight and had their strange,\n",
      "    With his sward shall strave a servant state.\n",
      "    Where this't she is that to the wind of held\n",
      "    That have this serve that she he with the child\n",
      "    Which they were beauty of their command strowes\n",
      "    And truth and strength to the serves and song.\n",
      "    If thou say'st he that hath seen this should still\n",
      "    To she with his both shall see him.\n",
      "    The world was a solder thou to heaven with me,\n",
      "    And should this can stay that I heave make\n",
      "    Which his charge in her shames, and to his state.\n",
      "    That have tho stol'd of this starts to have,  \n",
      "    And we and to the cheeks that to the stol'd\n",
      "    To serve the courtier time of that sense is.\n",
      "    In the summer that that shall not,\n",
      "    That he will s\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9cd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
