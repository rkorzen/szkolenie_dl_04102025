{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation — wersja skrócona i praktyczna (TensorFlow i PyTorch)\n",
    "\n",
    "Ten notatnik został uproszczony. Na początku znajdziesz bardzo krótkie, działające przykłady k‑fold cross validation dla prostego problemu klasyfikacji (Iris) w TensorFlow/Keras i w PyTorch. Każdy przykład liczy accuracy dla każdego folda i rysuje prosty wykres z interpretacją. Dalej pozostawiono bardziej rozbudowane materiały dla chętnych.\n",
    "\n",
    "Czym jest k‑fold CV? Dzielimy dane na k równych części (foldów). Uczymy model k razy: za każdym razem na (k−1) foldach, a testujemy na pozostałym. Średnia z wyników to stabilniejsza estymacja jakości niż pojedynczy train/val split.\n",
    "\n",
    "Najważniejsze zasady w DL:\n",
    "- Nowy model dla każdego folda (nie przenosimy wag).\n",
    "- Normalizację/standaryzację dopasowujemy wyłącznie na danych treningowych folda, a potem transformujemy walidację.\n",
    "- Ustaw seedy (powtarzalność) i użyj krótkiego treningu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład 1: TensorFlow/Keras — krótka k‑fold CV na Iris\n",
    "\n",
    "- Dane: 150 próbek, 4 cechy, 3 klasy.\n",
    "- Model: niewielka sieć MLP (1 warstwa ukryta).\n",
    "- Trening: kilka epok na fold (szybko i lekko). \n",
    "\n",
    "Po każdym foldzie zapisujemy accuracy. Na końcu rysujemy wykres słupkowy i opisujemy interpretację.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: accuracy = 1.000\n",
      "Fold 2: accuracy = 0.967\n",
      "Fold 3: accuracy = 0.867\n",
      "Fold 4: accuracy = 1.000\n",
      "Fold 5: accuracy = 0.900\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.random.seed(42)\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_acc = []\n",
    "\n",
    "for i, (tr_idx, va_idx) in enumerate(kf.split(X, y), start=1):\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X[tr_idx])\n",
    "    X_va = scaler.transform(X[va_idx])\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X.shape[1],)),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_tr, y[tr_idx], epochs=20, batch_size=16, verbose=0)\n",
    "    preds = np.argmax(model.predict(X_va, verbose=0), axis=1)\n",
    "    acc = accuracy_score(y[va_idx], preds)\n",
    "    fold_acc.append(acc)\n",
    "    print(f\"Fold {i}: accuracy = {acc:.3f}\")\n",
    "\n",
    "mean_acc = np.mean(fold_acc)\n",
    "std_acc = np.std(fold_acc)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(range(1, k+1), fold_acc, color=\"#6A5ACD\")\n",
    "plt.axhline(mean_acc, color=\"red\", linestyle=\"--\", label=f\"Średnia = {mean_acc:.3f}\")\n",
    "plt.title(\"Keras: Accuracy na foldach (Iris)\")\n",
    "plt.xlabel(\"Numer folda\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretacja wykresu: Każdy słupek to accuracy na jednym foldzie (różne podziały uczą/walidują model). Czerwona linia przerywana to średnia accuracy — traktujemy ją jako główną miarę jakości modelu. Rozrzut słupków (odchylenie) informuje o wrażliwości na podział danych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład 2: PyTorch — krótka k‑fold CV na Iris\n",
    "\n",
    "Analogiczna procedura, ale z ręczną pętlą uczącą w PyTorch. Model to małe MLP, a na końcu rysujemy wykres accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def make_model(input_dim=4, hidden=16, num_classes=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, num_classes),\n",
    "    )\n",
    "\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_acc_pt = []\n",
    "\n",
    "for i, (tr_idx, va_idx) in enumerate(kf.split(X, y), start=1):\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X[tr_idx]).astype(np.float32)\n",
    "    X_va = scaler.transform(X[va_idx]).astype(np.float32)\n",
    "    y_tr = y[tr_idx].astype(np.int64)\n",
    "    y_va = y[va_idx].astype(np.int64)\n",
    "\n",
    "    train_ds = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr))\n",
    "    val_ds = TensorDataset(torch.tensor(X_va), torch.tensor(y_va))\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = make_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_va))\n",
    "        preds = logits.argmax(dim=1).numpy()\n",
    "    acc = (preds == y_va).mean()\n",
    "    fold_acc_pt.append(acc)\n",
    "    print(f\"Fold {i}: accuracy = {acc:.3f}\")\n",
    "\n",
    "mean_acc_pt = float(np.mean(fold_acc_pt))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(range(1, k+1), fold_acc_pt, color=\"#20B2AA\")\n",
    "plt.axhline(mean_acc_pt, color=\"red\", linestyle=\"--\", label=f\"Średnia = {mean_acc_pt:.3f}\")\n",
    "plt.title(\"PyTorch: Accuracy na foldach (Iris)\")\n",
    "plt.xlabel(\"Numer folda\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretacja wykresu: Słupki pokazują wyniki na poszczególnych foldach w PyTorchu; czerwona linia to średnia. Jeśli rozrzut jest mały i średnia wysoka, model jest stabilny i dobrze generalizuje na tym zbiorze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Poniżej pozostawiono wcześniejszą, bardziej rozbudowaną część notatnika (sklearn, warianty itp.). Możesz ją pominąć, jeśli szukasz tylko krótkich przykładów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation w uczeniu klasycznym i deep learningu\n",
    "\n",
    "### Co jest dostępne od ręki, a co trzeba napisać samemu?\n",
    "\n",
    "1. **scikit-learn**\n",
    "   - Gotowe narzędzia: `cross_val_score`, `GridSearchCV`, `StratifiedKFold`\n",
    "   - Wystarczy przekazać estimator; biblioteka zadba o podziały i metryki\n",
    "\n",
    "2. **PyTorch**\n",
    "   - Brak wbudowanego CV; korzystamy z `KFold`/`StratifiedKFold` do dzielenia indeksów\n",
    "   - Dla każdego fold-u tworzymy świeży model, DataLoader i pętlę treningową\n",
    "   - Możemy łatwo sterować urządzeniem (CPU/GPU), funkcjami straty itd.\n",
    "\n",
    "3. **TensorFlow / Keras**\n",
    "   - Podobnie jak w PyTorch – implementujemy pętlę sami\n",
    "   - Keras udostępnia wygodny interfejs `model.fit`, więc w praktyce wystarczy pętla po foldach i ręczne przerzucanie danych\n",
    "\n",
    "### Najważniejsze zasady przy CV w deep learningu\n",
    "\n",
    "- **Nowy model dla każdego fold-u** – nie przenosimy wag pomiędzy foldami\n",
    "- **Oddzielna normalizacja** – transformacje (np. standaryzacja) dopasowujemy tylko na danych treningowych danego fold-u\n",
    "- **Kontrola losowości** – ustaw seedy dla NumPy / PyTorch / TensorFlow\n",
    "- **Monitorowanie uczenia** – warto logować przebieg strat i korzystać z early stoppingu\n",
    "- **Sprzątanie pamięci** – przy GPU warto po każdym foldzie wywołać `torch.cuda.empty_cache()` / `tf.keras.backend.clear_session()`\n",
    "\n",
    "Poniżej znajdziesz przykłady: najpierw klasyczne użycie scikit-learn, a następnie minimalistyczne implementacje dla TensorFlow/Keras oraz PyTorch – wszystko bez dodatkowych wrapperów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def keras_cross_validation(\n",
    "    X,\n",
    "    y,\n",
    "    k=5,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    lr=0.001,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    K-Fold Cross Validation dla modeli Keras\n",
    "    \"\"\"\n",
    "    # Przygotowanie danych\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y_categorical = to_categorical(y, num_classes=10)\n",
    "\n",
    "    # K-Fold\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"=== KERAS {k}-FOLD CROSS VALIDATION ===\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold + 1}/{k}...\")\n",
    "\n",
    "        # Podział danych\n",
    "        X_train_fold = X_scaled[train_idx]\n",
    "        X_val_fold = X_scaled[val_idx]\n",
    "        y_train_fold = y_categorical[train_idx]\n",
    "        y_val_fold = y_categorical[val_idx]\n",
    "\n",
    "        # Tworzenie nowego modelu dla każdego fold-a\n",
    "        model = create_keras_model(\n",
    "            input_dim=X_scaled.shape[1],\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        # Trening modelu\n",
    "        history = model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            verbose=0,  # Cicha praca\n",
    "        )\n",
    "\n",
    "        # Ewaluacja\n",
    "        val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        cv_scores.append(val_accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Fold {fold + 1} accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    cv_scores = np.array(cv_scores)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\\\nWyniki Cross Validation:\")\n",
    "        print(f\"Średnia accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(f\"Zakres: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]\")\n",
    "\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja Cross Validation dla TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(input_dim=64, hidden_size=128, dropout_rate=0.3, lr=0.001):\n",
    "    \"\"\"\n",
    "    Funkcja tworząca model Keras\n",
    "    \"\"\"\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(hidden_size, activation=\"relu\", input_dim=input_dim),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(hidden_size, activation=\"relu\"),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(10, activation=\"softmax\"),  # 10 klas\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Test modelu\n",
    "\n",
    "model = create_keras_model()\n",
    "print(\"Model TensorFlow/Keras:\")\n",
    "model.summary()\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = np.random.random((1, 64))\n",
    "output = model.predict(sample_input, verbose=0)\n",
    "print(f\"Kształt wyjścia: {output.shape}\")\n",
    "print(f\"Suma prawdopodobieństw: {output.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja modelu TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Import bibliotek TensorFlow/Keras\n",
    "\n",
    "\n",
    "print(f\"TensorFlow wersja: {tf.__version__}\")\n",
    "print(f\"GPU dostępne: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Wyłączenie ostrzeżeń TensorFlow\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# Przygotowanie danych dla TensorFlow\n",
    "if \"X_digits\" not in globals():\n",
    "    from sklearn.datasets import load_digits\n",
    "\n",
    "    digits = load_digits()\n",
    "    X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "X_digits_scaled = StandardScaler().fit_transform(X_digits)\n",
    "y_digits_categorical = to_categorical(y_digits, num_classes=10)\n",
    "\n",
    "print(f\"Kształt danych X: {X_digits_scaled.shape}\")\n",
    "print(f\"Kształt danych y: {y_digits_categorical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Cross Validation w TensorFlow/Keras\n",
    "\n",
    "TensorFlow/Keras również nie ma wbudowanego mechanizmu cross validation, ale można łatwo zintegrować sklearn's KFold z modelami Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class DigitsClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=64, dropout_rate=0.3):\n",
    "        super(DigitsClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(64, hidden_size),   # 64 cechy wejściowe w digits\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 10)    # 10 klas cyfr\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def pytorch_hyperparameter_search(X, y, param_grid, k=5):\n",
    "    \"\"\"\n",
    "    Grid search z Cross Validation dla PyTorch\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    print(\"=== PYTORCH HYPERPARAMETER SEARCH ===\")\n",
    "\n",
    "    # Generowanie wszystkich kombinacji parametrów\n",
    "    from itertools import product\n",
    "\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    for params in product(*param_values):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        print(f\"\\\\nTestowanie parametrów: {param_dict}\")\n",
    "\n",
    "        # Przekazanie parametrów do funkcji CV\n",
    "        scores = pytorch_cross_validation_params(X, y, k=k, **param_dict, verbose=False)\n",
    "        mean_score = scores.mean()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"params\": param_dict.copy(),\n",
    "                \"scores\": scores,\n",
    "                \"mean_score\": mean_score,\n",
    "                \"std_score\": scores.std(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Średni wynik: {mean_score:.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = param_dict.copy()\n",
    "\n",
    "    print(f\"\\\\n=== NAJLEPSZE PARAMETRY ===\")\n",
    "    print(f\"Parametry: {best_params}\")\n",
    "    print(f\"Wynik: {best_score:.4f}\")\n",
    "\n",
    "    return best_params, results\n",
    "\n",
    "\n",
    "def pytorch_cross_validation_params(\n",
    "    X,\n",
    "    y,\n",
    "    k=5,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    lr=0.001,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross validation z możliwością ustawiania parametrów\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_tensor = torch.FloatTensor(X_scaled)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"=== PYTORCH CV - Hidden: {hidden_size}, Dropout: {dropout_rate}, LR: {lr} ===\"\n",
    "        )\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold + 1}/{k}...\")\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Model z podanymi parametrami\n",
    "        model = DigitsClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(\n",
    "            device\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        model = train_model_pytorch(model, train_loader, criterion, optimizer, epochs)\n",
    "        accuracy = evaluate_model_pytorch(model, test_loader)\n",
    "        cv_scores.append(accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Fold {fold + 1} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    cv_scores = np.array(cv_scores)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Średnia accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    return cv_scores\n",
    "\n",
    "def train_model_pytorch(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def evaluate_model_pytorch(model, test_loader):\n",
    "    \"\"\"\n",
    "    Ocena modelu na zbiorze testowym.\n",
    "    Zwraca accuracy (odsetek poprawnych klasyfikacji).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Definicja siatki parametrów\n",
    "param_grid = {\n",
    "    \"hidden_size\": [64, 128, 256],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.5],\n",
    "    \"lr\": [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Uruchomienie grid search (ograniczone dla czasu wykonania)\n",
    "print(\"Uruchamianie skróconej wersji grid search...\")\n",
    "limited_param_grid = {\n",
    "    \"hidden_size\": [64, 128],\n",
    "    \"dropout_rate\": [0.3],\n",
    "    \"lr\": [0.001, 0.01],\n",
    "}\n",
    "\n",
    "best_params, search_results = pytorch_hyperparameter_search(\n",
    "    X_digits, y_digits, limited_param_grid, k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hiperparametrów w PyTorch z Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie z modelami scikit-learn na tym samym zbiorze danych\n",
    "sklearn_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "}\n",
    "\n",
    "sklearn_results = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Obliczenie wyników dla PyTorch NN\n",
    "pytorch_scores = pytorch_cross_validation_params(\n",
    "    X_digits,\n",
    "    y_digits,\n",
    "    k=5,                 # liczba foldów (żeby było porównywalne ze sklearn)\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    lr=0.001,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"=== PORÓWNANIE PYTORCH Z SCIKIT-LEARN ===\")\n",
    "\n",
    "# Cross validation dla modeli sklearn\n",
    "for name, model in sklearn_models.items():\n",
    "    # Normalizacja dla SVM i Logistic Regression\n",
    "    data_to_use = (\n",
    "        StandardScaler().fit_transform(X_digits)\n",
    "        if name in [\"SVM\", \"Logistic Regression\"]\n",
    "        else X_digits\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(model, data_to_use, y_digits, cv=skf, scoring=\"accuracy\")\n",
    "    sklearn_results[name] = scores\n",
    "\n",
    "    print(f\"{name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "print(f\"PyTorch NN: {pytorch_scores.mean():.4f} ± {pytorch_scores.std():.4f}\")\n",
    "\n",
    "# Wizualizacja porównania\n",
    "all_results = sklearn_results.copy()\n",
    "all_results[\"PyTorch NN\"] = pytorch_scores\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(all_results.values(), labels=all_results.keys())\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Porównanie PyTorch z scikit-learn (Digits dataset)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ranking modeli\n",
    "model_ranking = sorted(all_results.items(), key=lambda x: x[1].mean(), reverse=True)\n",
    "print(\"\\n=== RANKING MODELI ===\")\n",
    "for i, (name, scores) in enumerate(model_ranking):\n",
    "    print(f\"{i+1}. {name}: {scores.mean():.4f} ± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie z scikit-learn na tym samym zbiorze danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def train_model_pytorch(model, train_loader, criterion, optimizer, epochs=50):\n",
    "    \"\"\"\n",
    "    Funkcja trenująca model PyTorch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model_pytorch(model, test_loader):\n",
    "    \"\"\"\n",
    "    Funkcja ewaluująca model PyTorch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def pytorch_cross_validation(X, y, k=5, epochs=50, batch_size=32, lr=0.001):\n",
    "    \"\"\"\n",
    "    Implementacja K-Fold Cross Validation dla PyTorch\n",
    "    \"\"\"\n",
    "    # Normalizacja danych\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Konwersja do tensorów\n",
    "    X_tensor = torch.FloatTensor(X_scaled)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    # Tworzenie datasetu\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    \n",
    "    # K-Fold\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"=== PYTORCH {k}-FOLD CROSS VALIDATION ===\")\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Fold {fold + 1}/{k}...\")\n",
    "        \n",
    "        # Tworzenie subset-ów\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        test_subset = Subset(dataset, test_idx)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Nowy model dla każdego fold-a\n",
    "        model = DigitsClassifier().to(device)      \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Trening\n",
    "        model = train_model_pytorch(model, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        # Ewaluacja\n",
    "        accuracy = evaluate_model_pytorch(model, test_loader)\n",
    "        cv_scores.append(accuracy)\n",
    "        \n",
    "        print(f\"  Fold {fold + 1} accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    cv_scores = np.array(cv_scores)\n",
    "    print(f\"\\nWyniki Cross Validation:\")\n",
    "    print(f\"Średnia accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    print(f\"Zakres: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Uruchomienie PyTorch Cross Validation\n",
    "pytorch_scores = pytorch_cross_validation(X_digits, y_digits, k=5, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie wyników PyTorch i TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras_scores = keras_cross_validation(X_digits, y_digits, k=5, epochs=30)\n",
    "print(\"=== PORÓWNANIE PYTORCH VS KERAS ===\")\n",
    "print(f\"PyTorch NN: {pytorch_scores.mean():.4f} ± {pytorch_scores.std():.4f}\")\n",
    "print(f\"Keras NN:   {keras_scores.mean():.4f} ± {keras_scores.std():.4f}\")\n",
    "\n",
    "frameworks_comparison = {\n",
    "    'PyTorch': pytorch_scores,\n",
    "    'TensorFlow/Keras': keras_scores,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(frameworks_comparison.values(), labels=frameworks_comparison.keys())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Porównanie PyTorch vs TensorFlow/Keras')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja Cross Validation dla PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=64, hidden_size=128, num_classes=10, dropout_rate=0.3\n",
    "    ):\n",
    "        super(DigitsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test modelu\n",
    "model = DigitsClassifier()\n",
    "print(\"Model PyTorch:\")\n",
    "print(model)\n",
    "print(f\"Liczba parametrów: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Przykład forward pass\n",
    "sample_input = torch.randn(1, 64)\n",
    "output = model(sample_input)\n",
    "print(f\"Kształt wyjścia: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja modelu PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dodatkowych bibliotek dla PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "\n",
    "# Sprawdźmy czy CUDA jest dostępne\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "# Ładowanie większego zbioru danych - MNIST-like digits\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(f\"Kształt danych Digits: {X_digits.shape}\")\n",
    "print(f\"Liczba klas: {len(np.unique(y_digits))}\")\n",
    "print(f\"Zakres pikseli: [{X_digits.min()}, {X_digits.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross Validation w PyTorch\n",
    "\n",
    "W PyTorch nie ma wbudowanych narzędzi do cross validation jak w scikit-learn, ale możemy łatwo zaimplementować tę funkcjonalność używając `KFold` z sklearn oraz PyTorch-owych modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (Walidacja Krzyżowa)\n",
    "\n",
    "## Co to jest Cross Validation?\n",
    "\n",
    "Cross Validation (walidacja krzyżowa) to technika ewaluacji modeli uczenia maszynowego, która pozwala na bardziej wiarygodną ocenę wydajności modelu poprzez wielokrotne dzielenie danych na zbiory treningowe i walidacyjne.\n",
    "\n",
    "## Dlaczego Cross Validation jest ważna?\n",
    "\n",
    "### Problemy z prostym podziałem train/test:\n",
    "- **Zależność od konkretnego podziału** - wynik może się różnić w zależności od tego, które dane trafiły do zbioru testowego\n",
    "- **Mała reprezentatywność** - szczególnie przy małych zbiorach danych\n",
    "- **Overfitting do zbioru testowego** - gdy wielokrotnie testujemy różne modele na tym samym zbiorze\n",
    "\n",
    "### Zalety Cross Validation:\n",
    "- **Bardziej stabilne wyniki** - każda próbka jest używana zarówno do treningu jak i testowania\n",
    "- **Lepsza ocena wydajności** - otrzymujemy średnią i odchylenie standardowe\n",
    "- **Efektywne wykorzystanie danych** - szczególnie ważne przy małych zbiorach\n",
    "- **Redukcja overfittingu** - model jest testowany na różnych podzbiorach danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    learning_curve,\n",
    "    train_test_split,\n",
    "    validation_curve,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ustawienia dla wykresów\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ładowanie różnych zbiorów danych\n",
    "iris = load_iris()\n",
    "wine = load_wine()\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Dla przykładu użyjemy zbioru Iris\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Kształt danych: {X.shape}\")\n",
    "print(f\"Liczba klas: {len(np.unique(y))}\")\n",
    "print(f\"Klasy: {iris.target_names}\")\n",
    "print(f\"Cechy: {iris.feature_names}\")\n",
    "\n",
    "# Sprawdźmy rozkład klas\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nRozkład klas: {dict(zip(iris.target_names, counts))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Porównanie: Prosty podział vs Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prosty podział train/test\n",
    "def simple_train_test_split_evaluation(X, y, n_splits=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Porównanie wyników przy różnych losowych podziałach train/test\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=i\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "def cross_validation_evaluation(X, y, cv=10):\n",
    "    \"\"\"\n",
    "    Ewaluacja używając cross validation\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Porównanie wyników\n",
    "simple_scores = simple_train_test_split_evaluation(X, y)\n",
    "cv_scores = cross_validation_evaluation(X, y)\n",
    "\n",
    "print(\"=== PORÓWNANIE METOD EWALUACJI ===\")\n",
    "print(f\"\\nProsty podział train/test (10 różnych podziałów):\")\n",
    "print(f\"Średnia accuracy: {simple_scores.mean():.4f} ± {simple_scores.std():.4f}\")\n",
    "print(f\"Zakres: [{simple_scores.min():.4f}, {simple_scores.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n10-Fold Cross Validation:\")\n",
    "print(f\"Średnia accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print(f\"Zakres: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]\")\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([simple_scores, cv_scores], labels=[\"Train/Test Split\", \"Cross Validation\"])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Porównanie stabilności wyników\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    range(1, len(simple_scores) + 1),\n",
    "    simple_scores,\n",
    "    \"o-\",\n",
    "    label=\"Train/Test Split\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(cv_scores) + 1), cv_scores, \"s-\", label=\"Cross Validation\", alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"Iteracja/Fold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Wyniki w poszczególnych iteracjach\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rodzaje Cross Validation\n",
    "\n",
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kfold_splits(X, y, k=5):\n",
    "    \"\"\"\n",
    "    Wizualizacja podziałów w K-Fold CV\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    fig, axes = plt.subplots(k, 1, figsize=(12, 2 * k))\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Tworzenie wizualizacji podziału\n",
    "        split_viz = np.zeros(len(X))\n",
    "        split_viz[train_idx] = 1  # trening = 1\n",
    "        split_viz[test_idx] = 2  # test = 2\n",
    "\n",
    "        # Kolorowanie według klas\n",
    "        colors = [\"lightgray\", \"lightblue\", \"lightcoral\"]\n",
    "\n",
    "        axes[fold].scatter(\n",
    "            range(len(X)),\n",
    "            [fold] * len(X),\n",
    "            c=[colors[int(val)] for val in split_viz],\n",
    "            s=50,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        axes[fold].set_title(\n",
    "            f\"Fold {fold + 1}: Train ({len(train_idx)} próbek) vs Test ({len(test_idx)} próbek)\"\n",
    "        )\n",
    "        axes[fold].set_xlabel(\"Indeks próbki\")\n",
    "        axes[fold].set_yticks([])\n",
    "        axes[fold].grid(True, alpha=0.3)\n",
    "\n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightblue\", label=\"Trening\"),\n",
    "        Patch(facecolor=\"lightcoral\", label=\"Test\"),\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Wizualizacja 5-Fold CV\n",
    "print(\"=== K-FOLD CROSS VALIDATION ===\")\n",
    "print(\"Wizualizacja podziałów w 5-Fold Cross Validation:\")\n",
    "visualize_kfold_splits(X, y, k=5)\n",
    "\n",
    "# Porównanie różnych wartości k\n",
    "k_values = [3, 5, 10, 15]\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "results = []\n",
    "for k in k_values:\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=\"accuracy\")\n",
    "    results.append(\n",
    "        {\n",
    "            \"k\": k,\n",
    "            \"mean_score\": scores.mean(),\n",
    "            \"std_score\": scores.std(),\n",
    "            \"scores\": scores,\n",
    "        }\n",
    "    )\n",
    "    print(f\"{k}-Fold CV: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Wizualizacja wyników dla różnych k\n",
    "plt.figure(figsize=(10, 6))\n",
    "means = [r[\"mean_score\"] for r in results]\n",
    "stds = [r[\"std_score\"] for r in results]\n",
    "\n",
    "plt.errorbar(k_values, means, yerr=stds, fmt=\"o-\", capsize=5, capthick=2)\n",
    "plt.xlabel(\"Liczba foldów (k)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Wpływ liczby foldów na wyniki Cross Validation\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kfold_vs_stratified(X, y, k=5):\n",
    "    \"\"\"\n",
    "    Porównanie zwykłego K-Fold z Stratified K-Fold\n",
    "    \"\"\"\n",
    "    # Zwykły K-Fold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"=== PORÓWNANIE K-FOLD vs STRATIFIED K-FOLD ===\")\n",
    "    print(f\"\\nOryginalny rozkład klas: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, k, figsize=(3 * k, 6))\n",
    "\n",
    "    # K-Fold\n",
    "    print(\"\\n--- Zwykły K-Fold ---\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "        test_classes = np.unique(y[test_idx], return_counts=True)\n",
    "        class_distribution = dict(zip(test_classes[0], test_classes[1]))\n",
    "        print(f\"Fold {fold+1} - Rozkład w zbiorze testowym: {class_distribution}\")\n",
    "\n",
    "        # Wizualizacja\n",
    "        axes[0, fold].bar(\n",
    "            range(len(iris.target_names)),\n",
    "            [class_distribution.get(i, 0) for i in range(len(iris.target_names))],\n",
    "        )\n",
    "        axes[0, fold].set_title(f\"K-Fold\\nFold {fold+1}\")\n",
    "        axes[0, fold].set_xticks(range(len(iris.target_names)))\n",
    "        axes[0, fold].set_xticklabels(iris.target_names, rotation=45)\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    print(\"\\n--- Stratified K-Fold ---\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        test_classes = np.unique(y[test_idx], return_counts=True)\n",
    "        class_distribution = dict(zip(test_classes[0], test_classes[1]))\n",
    "        print(f\"Fold {fold+1} - Rozkład w zbiorze testowym: {class_distribution}\")\n",
    "\n",
    "        # Wizualizacja\n",
    "        axes[1, fold].bar(\n",
    "            range(len(iris.target_names)),\n",
    "            [class_distribution.get(i, 0) for i in range(len(iris.target_names))],\n",
    "        )\n",
    "        axes[1, fold].set_title(f\"Stratified\\nFold {fold+1}\")\n",
    "        axes[1, fold].set_xticks(range(len(iris.target_names)))\n",
    "        axes[1, fold].set_xticklabels(iris.target_names, rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Porównanie wyników\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    kf_scores = cross_val_score(model, X, y, cv=kf, scoring=\"accuracy\")\n",
    "    skf_scores = cross_val_score(model, X, y, cv=skf, scoring=\"accuracy\")\n",
    "\n",
    "    print(f\"\\n--- Wyniki ---\")\n",
    "    print(f\"K-Fold CV: {kf_scores.mean():.4f} ± {kf_scores.std():.4f}\")\n",
    "    print(f\"Stratified K-Fold CV: {skf_scores.mean():.4f} ± {skf_scores.std():.4f}\")\n",
    "\n",
    "    return kf_scores, skf_scores\n",
    "\n",
    "\n",
    "kf_scores, skf_scores = compare_kfold_vs_stratified(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross Validation dla różnych modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja różnych modeli\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "}\n",
    "\n",
    "# Standardyzacja danych (ważne dla niektórych modeli)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cross validation dla każdego modelu\n",
    "cv_results = {}\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"=== PORÓWNANIE MODELI PRZY UŻYCIU CROSS VALIDATION ===\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Dla SVM i Logistic Regression używamy danych znormalizowanych\n",
    "    data_to_use = X_scaled if name in [\"SVM\", \"Logistic Regression\"] else X\n",
    "\n",
    "    scores = cross_val_score(model, data_to_use, y, cv=skf, scoring=\"accuracy\")\n",
    "    cv_results[name] = scores\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Średnia accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "    print(f\"  Zakres: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "    print()\n",
    "\n",
    "# Wizualizacja wyników\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(cv_results.values(), labels=cv_results.keys())\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Rozkład wyników Cross Validation\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Wyniki dla każdego fold-a\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, (name, scores) in enumerate(cv_results.items()):\n",
    "    plt.plot(range(1, len(scores) + 1), scores, \"o-\", label=name, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Wyniki w poszczególnych fold-ach\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabela podsumowująca\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": list(cv_results.keys()),\n",
    "        \"Mean Accuracy\": [scores.mean() for scores in cv_results.values()],\n",
    "        \"Std Accuracy\": [scores.std() for scores in cv_results.values()],\n",
    "        \"Min Accuracy\": [scores.min() for scores in cv_results.values()],\n",
    "        \"Max Accuracy\": [scores.max() for scores in cv_results.values()],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n=== TABELA PODSUMOWUJĄCA ===\")\n",
    "print(summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation Curve - tuning hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Curve dla Random Forest - liczba drzew\n",
    "param_range = [10, 50, 100, 150, 200, 250, 300]\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    X,\n",
    "    y,\n",
    "    param_name=\"n_estimators\",\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Obliczanie średnich i odchyleń standardowych\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_range, train_mean, \"o-\", color=\"blue\", label=\"Accuracy treningowa\")\n",
    "plt.fill_between(\n",
    "    param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\"\n",
    ")\n",
    "\n",
    "plt.plot(param_range, test_mean, \"o-\", color=\"red\", label=\"Accuracy walidacyjna\")\n",
    "plt.fill_between(\n",
    "    param_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Liczba drzew (n_estimators)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Curve - Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"=== VALIDATION CURVE - RANDOM FOREST ===\")\n",
    "for i, n_est in enumerate(param_range):\n",
    "    print(\n",
    "        f\"n_estimators={n_est}: Train={train_mean[i]:.4f}±{train_std[i]:.4f}, \"\n",
    "        f\"Validation={test_mean[i]:.4f}±{test_std[i]:.4f}\"\n",
    "    )\n",
    "\n",
    "# Validation Curve dla SVM - parametr C\n",
    "param_range_svm = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "train_scores_svm, test_scores_svm = validation_curve(\n",
    "    SVC(random_state=42),\n",
    "    X_scaled,\n",
    "    y,\n",
    "    param_name=\"C\",\n",
    "    param_range=param_range_svm,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "train_mean_svm = np.mean(train_scores_svm, axis=1)\n",
    "train_std_svm = np.std(train_scores_svm, axis=1)\n",
    "test_mean_svm = np.mean(test_scores_svm, axis=1)\n",
    "test_std_svm = np.std(test_scores_svm, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(\n",
    "    param_range_svm, train_mean_svm, \"o-\", color=\"blue\", label=\"Accuracy treningowa\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range_svm,\n",
    "    train_mean_svm - train_std_svm,\n",
    "    train_mean_svm + train_std_svm,\n",
    "    alpha=0.1,\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.semilogx(\n",
    "    param_range_svm, test_mean_svm, \"o-\", color=\"red\", label=\"Accuracy walidacyjna\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range_svm,\n",
    "    test_mean_svm - test_std_svm,\n",
    "    test_mean_svm + test_std_svm,\n",
    "    alpha=0.1,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Parametr C (skala logarytmiczna)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Curve - SVM\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== VALIDATION CURVE - SVM ===\")\n",
    "for i, c in enumerate(param_range_svm):\n",
    "    print(\n",
    "        f\"C={c}: Train={train_mean_svm[i]:.4f}±{train_std_svm[i]:.4f}, \"\n",
    "        f\"Validation={test_mean_svm[i]:.4f}±{test_std_svm[i]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curve - analiza wielkości zbioru treningowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve pokazuje jak wydajność modelu zmienia się w zależności od wielkości zbioru treningowego\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Random Forest\n",
    "train_sizes_abs, train_scores_lc, test_scores_lc = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X,\n",
    "    y,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "train_mean_lc = np.mean(train_scores_lc, axis=1)\n",
    "train_std_lc = np.std(train_scores_lc, axis=1)\n",
    "test_mean_lc = np.mean(test_scores_lc, axis=1)\n",
    "test_std_lc = np.std(test_scores_lc, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Random Forest Learning Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    train_sizes_abs, train_mean_lc, \"o-\", color=\"blue\", label=\"Accuracy treningowa\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    train_sizes_abs,\n",
    "    train_mean_lc - train_std_lc,\n",
    "    train_mean_lc + train_std_lc,\n",
    "    alpha=0.1,\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.plot(train_sizes_abs, test_mean_lc, \"o-\", color=\"red\", label=\"Accuracy walidacyjna\")\n",
    "plt.fill_between(\n",
    "    train_sizes_abs,\n",
    "    test_mean_lc - test_std_lc,\n",
    "    test_mean_lc + test_std_lc,\n",
    "    alpha=0.1,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Liczba próbek treningowych\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# SVM Learning Curve\n",
    "train_sizes_abs_svm, train_scores_lc_svm, test_scores_lc_svm = learning_curve(\n",
    "    SVC(random_state=42),\n",
    "    X_scaled,\n",
    "    y,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "train_mean_lc_svm = np.mean(train_scores_lc_svm, axis=1)\n",
    "train_std_lc_svm = np.std(train_scores_lc_svm, axis=1)\n",
    "test_mean_lc_svm = np.mean(test_scores_lc_svm, axis=1)\n",
    "test_std_lc_svm = np.std(test_scores_lc_svm, axis=1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    train_sizes_abs_svm,\n",
    "    train_mean_lc_svm,\n",
    "    \"o-\",\n",
    "    color=\"blue\",\n",
    "    label=\"Accuracy treningowa\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    train_sizes_abs_svm,\n",
    "    train_mean_lc_svm - train_std_lc_svm,\n",
    "    train_mean_lc_svm + train_std_lc_svm,\n",
    "    alpha=0.1,\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    train_sizes_abs_svm,\n",
    "    test_mean_lc_svm,\n",
    "    \"o-\",\n",
    "    color=\"red\",\n",
    "    label=\"Accuracy walidacyjna\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    train_sizes_abs_svm,\n",
    "    test_mean_lc_svm - test_std_lc_svm,\n",
    "    test_mean_lc_svm + test_std_lc_svm,\n",
    "    alpha=0.1,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Liczba próbek treningowych\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - SVM\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== ANALIZA LEARNING CURVES ===\")\n",
    "print(\"\\nRandom Forest:\")\n",
    "for i, size in enumerate(train_sizes_abs):\n",
    "    gap = train_mean_lc[i] - test_mean_lc[i]\n",
    "    print(\n",
    "        f\"Rozmiar: {size:3d}, Train: {train_mean_lc[i]:.4f}, \"\n",
    "        f\"Val: {test_mean_lc[i]:.4f}, Gap: {gap:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "for i, size in enumerate(train_sizes_abs_svm):\n",
    "    gap = train_mean_lc_svm[i] - test_mean_lc_svm[i]\n",
    "    print(\n",
    "        f\"Rozmiar: {size:3d}, Train: {train_mean_lc_svm[i]:.4f}, \"\n",
    "        f\"Val: {test_mean_lc_svm[i]:.4f}, Gap: {gap:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Praktyczny przykład: Wybór najlepszego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testowanie różnych modeli z różnymi hiperparametrami\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definicja modeli i ich hiperparametrów do testowania\n",
    "models_grid = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [3, 5, None],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "        },\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"rbf\", \"linear\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "        },\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"params\": {\"C\": [0.1, 1, 10], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "cv_scores_grid = {}\n",
    "\n",
    "print(\"=== GRID SEARCH Z CROSS VALIDATION ===\")\n",
    "print(\"Szukanie najlepszych hiperparametrów dla każdego modelu...\\n\")\n",
    "\n",
    "for name, config in models_grid.items():\n",
    "    print(f\"Testowanie {name}...\")\n",
    "\n",
    "    # Wybieranie odpowiednich danych (znormalizowane dla SVM i LR)\n",
    "    data_to_use = X_scaled if name in [\"SVM\", \"Logistic Regression\"] else X\n",
    "\n",
    "    # Grid Search z Cross Validation\n",
    "    grid_search = GridSearchCV(\n",
    "        config[\"model\"], config[\"params\"], cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(data_to_use, y)\n",
    "\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    cv_scores_grid[name] = grid_search.best_score_\n",
    "\n",
    "    print(f\"  Najlepsze parametry: {grid_search.best_params_}\")\n",
    "    print(f\"  Najlepszy wynik CV: {grid_search.best_score_:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Finalne porównanie najlepszych modeli\n",
    "print(\"=== FINALNE PORÓWNANIE NAJLEPSZYCH MODELI ===\")\n",
    "final_results = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    data_to_use = X_scaled if name in [\"SVM\", \"Logistic Regression\"] else X\n",
    "\n",
    "    # 10-fold CV dla finalnych wyników\n",
    "    scores = cross_val_score(model, data_to_use, y, cv=10, scoring=\"accuracy\")\n",
    "    final_results[name] = scores\n",
    "\n",
    "    print(f\"{name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Wizualizacja finalnych wyników\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(final_results.values(), labels=final_results.keys())\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Porównanie najlepszych modeli (po Grid Search)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wybór najlepszego modelu\n",
    "best_model_name = max(final_results.keys(), key=lambda x: final_results[x].mean())\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\n=== NAJLEPSZY MODEL: {best_model_name} ===\")\n",
    "print(\n",
    "    f\"Średnia accuracy: {final_results[best_model_name].mean():.4f} ± {final_results[best_model_name].std():.4f}\"\n",
    ")\n",
    "print(f\"Parametry modelu: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test na niezależnym zbiorze danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po wyborze najlepszego modelu, testujemy go na niezależnym zbiorze testowym\n",
    "print(\"=== TEST NA NIEZALEŻNYM ZBIORZE DANYCH ===\")\n",
    "\n",
    "# Podział na zbiór do CV i końcowy zbiór testowy\n",
    "X_cv, X_final_test, y_cv, y_final_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalizacja jeśli potrzebna\n",
    "if best_model_name in [\"SVM\", \"Logistic Regression\"]:\n",
    "    scaler_final = StandardScaler()\n",
    "    X_cv_scaled = scaler_final.fit_transform(X_cv)\n",
    "    X_final_test_scaled = scaler_final.transform(X_final_test)\n",
    "\n",
    "    X_cv_to_use = X_cv_scaled\n",
    "    X_test_to_use = X_final_test_scaled\n",
    "else:\n",
    "    X_cv_to_use = X_cv\n",
    "    X_test_to_use = X_final_test\n",
    "\n",
    "# Trening najlepszego modelu na zbiorze CV\n",
    "final_model = best_models[best_model_name]\n",
    "final_model.fit(X_cv_to_use, y_cv)\n",
    "\n",
    "# Predykcja na końcowym zbiorze testowym\n",
    "y_pred_final = final_model.predict(X_test_to_use)\n",
    "final_accuracy = accuracy_score(y_final_test, y_pred_final)\n",
    "\n",
    "print(\n",
    "    f\"Wynik Cross Validation: {final_results[best_model_name].mean():.4f} ± {final_results[best_model_name].std():.4f}\"\n",
    ")\n",
    "print(f\"Wynik na końcowym zbiorze testowym: {final_accuracy:.4f}\")\n",
    "print(f\"Różnica: {abs(final_results[best_model_name].mean() - final_accuracy):.4f}\")\n",
    "\n",
    "print(\"\\n=== SZCZEGÓŁOWY RAPORT KLASYFIKACJI ===\")\n",
    "print(classification_report(y_final_test, y_pred_final, target_names=iris.target_names))\n",
    "\n",
    "# Macierz konfuzji\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_final_test, y_pred_final)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=iris.target_names,\n",
    "    yticklabels=iris.target_names,\n",
    ")\n",
    "plt.title(\"Macierz konfuzji - Test finalny\")\n",
    "plt.ylabel(\"Rzeczywiste klasy\")\n",
    "plt.xlabel(\"Predykowane klasy\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== PODSUMOWANIE ===\")\n",
    "print(f\"Najlepszy model: {best_model_name}\")\n",
    "print(f\"Cross Validation pozwoliło nam wybrać najlepszy model i hiperparametry\")\n",
    "print(\n",
    "    f\"Wynik CV ({final_results[best_model_name].mean():.4f}) jest bardzo zbliżony do wyniku końcowego ({final_accuracy:.4f})\"\n",
    ")\n",
    "print(f\"To oznacza, że Cross Validation dobrze oszacowała wydajność modelu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Cross Validation\n",
    "\n",
    "### Główne zalety Cross Validation:\n",
    "\n",
    "1. **Bardziej wiarygodna ocena modelu**\n",
    "   - Każda próbka jest używana zarówno do treningu jak i testowania\n",
    "   - Otrzymujemy średnią i odchylenie standardowe wydajności\n",
    "\n",
    "2. **Efektywne wykorzystanie danych**\n",
    "   - Szczególnie ważne przy małych zbiorach danych\n",
    "   - Nie \"marnujemy\" danych na zbiór testowy\n",
    "\n",
    "3. **Redukcja overfittingu**\n",
    "   - Model jest testowany na różnych podzbiorach\n",
    "   - Trudniej \"zapamiętać\" konkretne próbki\n",
    "\n",
    "4. **Lepszy wybór hiperparametrów**\n",
    "   - Grid Search z CV daje bardziej stabilne wyniki\n",
    "   - Zmniejsza ryzyko overfittingu do zbioru walidacyjnego\n",
    "\n",
    "### Rodzaje Cross Validation:\n",
    "\n",
    "- **K-Fold**: Dzieli dane na k równych części\n",
    "- **Stratified K-Fold**: Zachowuje rozkład klas w każdym fold-zie\n",
    "- **Leave-One-Out (LOO)**: k = liczba próbek (bardzo kosztowne obliczeniowo)\n",
    "\n",
    "### Kiedy używać której metody:\n",
    "\n",
    "- **K-Fold (k=5 lub k=10)**: Standard dla większości problemów\n",
    "- **Stratified K-Fold**: Zawsze dla problemów klasyfikacji\n",
    "- **Leave-One-Out**: Tylko przy bardzo małych zbiorach danych\n",
    "\n",
    "### Najlepsze praktyki:\n",
    "\n",
    "1. **Zawsze używaj Stratified K-Fold dla klasyfikacji**\n",
    "2. **Ustaw random_state dla reprodukowalności**\n",
    "3. **Używaj CV do wyboru hiperparametrów, nie do końcowej oceny**\n",
    "4. **Zachowaj niezależny zbiór testowy na końcową ewaluację**\n",
    "5. **Pamiętaj o normalizacji danych (fit tylko na zbiorze treningowym!)**\n",
    "\n",
    "### Typowy workflow:\n",
    "\n",
    "1. Podział danych na zbiór do eksperymentów i końcowy zbiór testowy\n",
    "2. Użycie CV na zbiorze eksperymentalnym do:\n",
    "   - Porównania różnych modeli\n",
    "   - Tuningu hiperparametrów\n",
    "3. Wybór najlepszego modelu\n",
    "4. Trening na całym zbiorze eksperymentalnym\n",
    "5. **Jednokrotny** test na końcowym zbiorze testowym\n",
    "\n",
    "Cross Validation to podstawowe narzędzie w Machine Learning, które znacząco poprawia jakość i wiarygodność oceny modeli!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
