{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradienty w PyTorch\n",
    "W tej sekcji omawiamy implementacjƒô spadku gradientowego w PyTorch przy u≈ºyciu modu≈Çu <a href='https://pytorch.org/docs/stable/autograd.html'>`autograd`</a>. Wykorzystamy narzƒôdzia:\n",
    "\n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward'>`torch.autograd.backward()`</a>\n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad'>`torch.autograd.grad()`</a>\n",
    "\n",
    "\n",
    "Na wstƒôpie musimy przypomnieƒá kr√≥tkio czym sƒÖ te rzeczy:\n",
    "\n",
    "### Funkcje aktywacji\n",
    "\n",
    "* **Skokowa (Heaviside)** ‚Äì bardzo prosta, zwraca `0` poni≈ºej progu i `1` powy≈ºej. Historyczna, ale nie nadaje siƒô do trenowania sieci (brak pochodnej).\n",
    "* **Sigmoidalna (œÉ)** ‚Äì odwzorowuje wej≈õcie w zakres `(0,1)`. U≈ºywana jako model ‚Äûprawdopodobie≈Ñstwa‚Äù. Dobrze g≈Çadka, ale przy du≈ºych warto≈õciach wej≈õciowych gradient zanika.\n",
    "\n",
    "---\n",
    "\n",
    "### Kodowanie one-hot\n",
    "\n",
    "Reprezentacja kategorii jako wektora, w kt√≥rym jedna pozycja ma warto≈õƒá `1`, a pozosta≈Çe `0`.\n",
    "Np. klasa ‚Äûkot‚Äù przy trzech klasach: `[1,0,0]`.\n",
    "\n",
    "---\n",
    "\n",
    "### Maksymalne prawdopodobie≈Ñstwo (argmax)\n",
    "\n",
    "W klasyfikacji model zwraca rozk≈Çad prawdopodobie≈Ñstwa po klasach (np. przez **softmax**). Klasa przewidywana to ta o najwiƒôkszym prawdopodobie≈Ñstwie (`argmax`).\n",
    "\n",
    "---\n",
    "\n",
    "### Entropia krzy≈ºowa\n",
    "\n",
    "Miara r√≥≈ºnicy miƒôdzy **rozk≈Çadem przewidywanym** a **rozk≈Çadem rzeczywistym**.\n",
    "\n",
    "* **Binary cross-entropy** ‚Äì dla problem√≥w 0/1.\n",
    "* **Categorical cross-entropy** ‚Äì dla wielu klas (por√≥wnuje przewidywany rozk≈Çad softmax z etykietƒÖ one-hot).\n",
    "  Ni≈ºsza warto≈õƒá oznacza lepsze dopasowanie.\n",
    "\n",
    "---\n",
    "\n",
    "### Propagacja wsteczna (backprop)\n",
    "\n",
    "Algorytm wyznaczania gradient√≥w w sieci neuronowej:\n",
    "\n",
    "1. Obliczamy b≈ÇƒÖd (loss).\n",
    "2. Rozchodzimy ten b≈ÇƒÖd wstecz przez warstwy, u≈ºywajƒÖc regu≈Çy ≈Ça≈Ñcuchowej dla pochodnych.\n",
    "3. U≈ºywamy gradient√≥w do aktualizacji wag (np. w metodzie SGD).\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Te pojƒôcia tworzƒÖ bazƒô do zrozumienia, jak PyTorch `autograd` automatycznie ≈õledzi operacje na tensorach, liczy pochodne i pozwala na efektywne uczenie modeli.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h3>Dodatkowe materia≈Çy:</h3>\n",
    "<strong><a href='https://pytorch.org/docs/stable/notes/autograd.html'>Notatki PyTorch:</a></strong>&nbsp;&nbsp;<font color=black>Mechanika Autograd</font></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd ‚Äì automatyczne r√≥≈ºniczkowanie\n",
    "We wcze≈õniejszych czƒô≈õciach tworzyli≈õmy tensory i wykonywali≈õmy na nich r√≥≈ºne operacje, ale nie zapisywali≈õmy ich sekwencji ani nie wyznaczali≈õmy pochodnej gotowej funkcji.\n",
    "\n",
    "W tej sekcji wprowadzimy pojƒôcie <em>dynamicznego grafu obliczeniowego</em>, kt√≥ry sk≈Çada siƒô ze wszystkich obiekt√≥w typu <em>Tensor</em> w sieci oraz <em>funkcji</em>, kt√≥re je utworzy≈Çy. Zauwa≈º, ≈ºe jedynie tensory wej≈õciowe tworzone przez nas samych nie majƒÖ skojarzonych obiekt√≥w Function.\n",
    "\n",
    "Pakiet PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> zapewnia automatyczne r√≥≈ºniczkowanie dla wszystkich operacji na tensorach. Dzieje siƒô tak, poniewa≈º operacje stajƒÖ siƒô atrybutami samych tensor√≥w. Gdy atrybut tensora <tt>.requires_grad</tt> ustawimy na True, tensor zaczyna ≈õledziƒá wszystkie wykonywane na nim dzia≈Çania. Po zako≈Ñczeniu sekwencji operacji mo≈ºemy wywo≈Çaƒá <tt>.backward()</tt>, aby obliczyƒá wszystkie gradienty automatycznie. Gradient tensora zostanie zsumowany w jego atrybucie <tt>.grad</tt>.\n",
    "\n",
    "Zobaczmy to w praktyce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagacja wsteczna w jednym kroku\n",
    "Zaczniemy od zastosowania pojedynczej funkcji wielomianowej $y = f(x)$ do tensora $x$. Nastƒôpnie wykonamy propagacjƒô wstecznƒÖ i wypiszemy gradient $\\frac {dy} {dx}$.\n",
    "\n",
    "$\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\\\\n",
    "Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$\n",
    "\n",
    "#### Krok 1. Wykonaj standardowe importy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:50:49.840813Z",
     "start_time": "2025-09-30T18:50:49.834827Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krok 2. Utw√≥rz tensor z ustawionym <tt>requires_grad</tt> na True\n",
    "Dziƒôki temu tensor zacznie ≈õledziƒá obliczenia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:50:51.383753Z",
     "start_time": "2025-09-30T18:50:51.378869Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:27.617080Z",
     "start_time": "2025-09-30T18:40:27.614940Z"
    }
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krok 3. Zdefiniuj funkcjƒô\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:28.741973Z",
     "start_time": "2025-09-30T18:40:28.735865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniewa≈º $y$ powsta≈Ço w wyniku operacji, ma skojarzonƒÖ funkcjƒô gradientu dostƒôpnƒÖ jako <tt>y.grad_fn</tt>.<br>\n",
    "Obliczenie $y$ przebiega nastƒôpujƒÖco:<br>\n",
    "\n",
    "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\n",
    "\n",
    "To warto≈õƒá $y$ dla $x=2$.\n",
    "\n",
    "#### Krok 4. Propagacja wsteczna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:30.223697Z",
     "start_time": "2025-09-30T18:40:30.133586Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krok 5. Wy≈õwietl uzyskany gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:31.629977Z",
     "start_time": "2025-09-30T18:40:31.627563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauwa≈º, ≈ºe <tt>x.grad</tt> jest atrybutem tensora $x$, dlatego nie u≈ºywamy nawias√≥w. Obliczenie ma postaƒá<br>\n",
    "\n",
    "$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$\n",
    "\n",
    "To nachylenie wielomianu w punkcie $(2,63)$.\n",
    "\n",
    "## Propagacja wsteczna w wielu krokach\n",
    "Teraz wykonajmy co≈õ bardziej z≈Ço≈ºonego, z warstwami $y$ i $z$ pomiƒôdzy $x$ a warstwƒÖ wyj≈õciowƒÖ $out$.\n",
    "#### 1. Utw√≥rz tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:38.251961Z",
     "start_time": "2025-09-30T18:40:38.247952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Utw√≥rz pierwszƒÖ warstwƒô z $y = 3x+2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:39.671745Z",
     "start_time": "2025-09-30T18:40:39.669077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 3*x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Utw√≥rz drugƒÖ warstwƒô z $z = 2y^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:40.937432Z",
     "start_time": "2025-09-30T18:40:40.934592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*y**2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ustaw wyj≈õcie jako ≈õredniƒÖ macierzy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:42.380151Z",
     "start_time": "2025-09-30T18:40:42.376874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Wykonaj propagacjƒô wstecznƒÖ, aby znale≈∫ƒá gradient $x$ wzglƒôdem <tt>out</tt>\n",
    "(Je≈õli widzisz ten skr√≥t po raz pierwszy, w.r.t. oznacza <em>with respect to</em>, czyli ‚Äûwzglƒôdem‚Äù.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:40:44.750550Z",
     "start_time": "2025-09-30T18:40:44.746697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powiniene≈õ zobaczyƒá macierz 2x3. Je≈õli nazwiemy ko≈Ñcowy tensor <tt>out</tt> jako \"$o$\", w√≥wczas mo≈ºemy obliczyƒá pochodnƒÖ czƒÖstkowƒÖ $o$ wzglƒôdem $x_i$ nastƒôpujƒÖco:<br>\n",
    "\n",
    "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
    "\n",
    "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
    "\n",
    "Aby wyznaczyƒá pochodnƒÖ $z_i$, korzystamy z <a href='https://en.wikipedia.org/wiki/Chain_rule'>regu≈Çy ≈Ça≈Ñcuchowej</a>, wed≈Çug kt√≥rej pochodna $f(g(x)) = f'(g(x))g'(x)$.<br>\n",
    "\n",
    "W naszym przypadku<br>\n",
    "\n",
    "$\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\\\\n",
    "g(x) &= 3x+2, &g'(x) = 3 \\\\\n",
    "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$\n",
    "\n",
    "Zatem<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wy≈ÇƒÖcz ≈õledzenie\n",
    "ZdarzajƒÖ siƒô sytuacje, w kt√≥rych nie chcemy lub nie musimy ≈õledziƒá historii oblicze≈Ñ.\n",
    "\n",
    "Mo≈ºesz ponownie ustawiƒá atrybut tensora <tt>requires_grad</tt> w miejscu, u≈ºywajƒÖc `.requires_grad_(True)` (lub False) w razie potrzeby.\n",
    "\n",
    "Podczas ewaluacji czƒôsto warto owinƒÖƒá operacje blokiem `with torch.no_grad():`\n",
    "\n",
    "Rzadziej stosowanƒÖ metodƒÖ jest wywo≈Çanie `.detach()` na tensorze, aby uniemo≈ºliwiƒá ≈õledzenie przysz≈Çych oblicze≈Ñ. To przydatne podczas klonowania tensora. Nam te≈º sie przyda w paru miejscach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
