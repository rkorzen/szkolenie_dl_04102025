{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optymizery w Deep Learning\n",
    "\n",
    "## Czym są optymizery?\n",
    "\n",
    "Optymizery to algorytmy używane do minimalizowania funkcji straty (loss function) poprzez aktualizację wag sieci neuronowej. Ich głównym zadaniem jest znajdowanie optymalnych parametrów modelu w procesie uczenia.\n",
    "\n",
    "### Podstawowe zasady działania:\n",
    "- **Gradient Descent**: Podstawowy algorytm optymalizacji, który wykorzystuje gradienty do określenia kierunku aktualizacji wag\n",
    "- **Learning Rate**: Wielkość kroku, z jakim poruszamy się w kierunku minimum\n",
    "- **Momentum**: Pomaga w przyspieszeniu konwergencji i unikaniu lokalnych minimów\n",
    "- **Adaptive Learning**: Automatyczne dostosowywanie learning rate dla różnych parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularne rodzaje optymizatorów\n",
    "\n",
    "### 1. Stochastic Gradient Descent (SGD)\n",
    "**Cechy:**\n",
    "- Najprostszy optymizer\n",
    "- Stały learning rate\n",
    "- Może mieć problemy z lokalnimi minimami\n",
    "- Dobrze sprawdza się z momentum\n",
    "\n",
    "**Kiedy używać:** Proste problemy, gdy chcemy mieć pełną kontrolę nad procesem uczenia\n",
    "\n",
    "### 2. Adam (Adaptive Moment Estimation)\n",
    "**Cechy:**\n",
    "- Kombinuje momentum i adaptive learning rate\n",
    "- Automatycznie dostosowuje learning rate dla każdego parametru\n",
    "- Szybka konwergencja\n",
    "- Dobrze radzi sobie z rzadkimi gradientami\n",
    "\n",
    "**Kiedy używać:** Większość problemów deep learning, szczególnie z dużymi sieciami\n",
    "\n",
    "### 3. RMSprop\n",
    "**Cechy:**\n",
    "- Adaptive learning rate\n",
    "- Dobrze radzi sobie z niestacjonarnymi celami\n",
    "- Mniej pamięci niż Adam\n",
    "\n",
    "**Kiedy używać:** RNN, problemy z niestacjonarnymi danymi\n",
    "\n",
    "### 4. AdaGrad\n",
    "**Cechy:**\n",
    "- Dostosowuje learning rate na podstawie historii gradientów\n",
    "- Learning rate maleje w czasie\n",
    "- Może przedwcześnie zatrzymać uczenie\n",
    "\n",
    "**Kiedy używać:** Problemy z rzadkimi cechami, NLP\n",
    "\n",
    "### 5. AdamW\n",
    "**Cechy:**\n",
    "- Poprawiona wersja Adam z prawidłową regularyzacją weight decay\n",
    "- Lepsza generalizacja\n",
    "- Popularna w Transformerach\n",
    "\n",
    "**Kiedy używać:** Duże modele językowe, Transformery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady w PyTorch\n",
    "\n",
    "### Problem regresji - przewidywanie cen domów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Przygotowanie danych\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Konwersja do tensorów PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "# Model dla regresji\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.linear3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# Porównanie różnych optymizatorów\n",
    "def train_model(optimizer_name, model, optimizer, X_train, y_train, epochs=100):\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            losses.append(loss.item())\n",
    "            print(f'{optimizer_name} - Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SGD z Momentum ===\n",
      "SGD - Epoch [0/100], Loss: 5.9667\n",
      "SGD - Epoch [20/100], Loss: 0.7996\n",
      "SGD - Epoch [40/100], Loss: 0.6355\n",
      "SGD - Epoch [60/100], Loss: 0.5832\n",
      "SGD - Epoch [80/100], Loss: 0.5451\n",
      "\n",
      "=== Adam ===\n",
      "Adam - Epoch [0/100], Loss: 4.9593\n",
      "Adam - Epoch [20/100], Loss: 2.8883\n",
      "Adam - Epoch [40/100], Loss: 1.3969\n",
      "Adam - Epoch [60/100], Loss: 0.8766\n",
      "Adam - Epoch [80/100], Loss: 0.7535\n",
      "\n",
      "=== RMSprop ===\n",
      "RMSprop - Epoch [0/100], Loss: 5.7923\n",
      "RMSprop - Epoch [20/100], Loss: 0.7798\n",
      "RMSprop - Epoch [40/100], Loss: 0.6209\n",
      "RMSprop - Epoch [60/100], Loss: 0.5361\n",
      "RMSprop - Epoch [80/100], Loss: 0.4874\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "model_sgd = RegressionModel(X_train.shape[1])\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01, momentum=0.9)\n",
    "print(\"=== SGD z Momentum ===\")\n",
    "sgd_losses = train_model(\"SGD\", model_sgd, optimizer_sgd, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Adam\n",
    "model_adam = RegressionModel(X_train.shape[1])\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001)\n",
    "print(\"\\n=== Adam ===\")\n",
    "adam_losses = train_model(\"Adam\", model_adam, optimizer_adam, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# RMSprop\n",
    "model_rmsprop = RegressionModel(X_train.shape[1])\n",
    "optimizer_rmsprop = optim.RMSprop(model_rmsprop.parameters(), lr=0.001)\n",
    "print(\"\\n=== RMSprop ===\")\n",
    "rmsprop_losses = train_model(\"RMSprop\", model_rmsprop, optimizer_rmsprop, X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem klasyfikacji - rozpoznawanie cyfr MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:03<00:00, 2.63MB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 110kB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 1.48MB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 2.21MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Przygotowanie danych MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Model CNN dla klasyfikacji\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "def train_classifier(model, optimizer, train_loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 300 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "\n",
    "def test_classifier(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkorzen/PycharmProjects/pytorch_for_DL_with_python_bootcamp/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1535: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Adam dla klasyfikacji ===\n",
      "Epoch 1, Batch 0, Loss: 2.313283\n",
      "Epoch 1, Batch 300, Loss: 0.212340\n",
      "Epoch 1, Batch 600, Loss: 0.113957\n",
      "Epoch 1, Batch 900, Loss: 0.077497\n",
      "Epoch 2, Batch 0, Loss: 0.083085\n",
      "Epoch 2, Batch 300, Loss: 0.199304\n",
      "Epoch 2, Batch 600, Loss: 0.095614\n",
      "Epoch 2, Batch 900, Loss: 0.139638\n",
      "Epoch 3, Batch 0, Loss: 0.059208\n",
      "Epoch 3, Batch 300, Loss: 0.064497\n",
      "Epoch 3, Batch 600, Loss: 0.069703\n",
      "Epoch 3, Batch 900, Loss: 0.039880\n",
      "Epoch 4, Batch 0, Loss: 0.063968\n",
      "Epoch 4, Batch 300, Loss: 0.116923\n",
      "Epoch 4, Batch 600, Loss: 0.008908\n",
      "Epoch 4, Batch 900, Loss: 0.138580\n",
      "Epoch 5, Batch 0, Loss: 0.062773\n",
      "Epoch 5, Batch 300, Loss: 0.023269\n",
      "Epoch 5, Batch 600, Loss: 0.026062\n",
      "Epoch 5, Batch 900, Loss: 0.004383\n",
      "Test Loss: 0.0309, Accuracy: 99.05%\n",
      "\n",
      "=== AdamW dla klasyfikacji ===\n",
      "Epoch 1, Batch 0, Loss: 2.284003\n",
      "Epoch 1, Batch 300, Loss: 0.155624\n",
      "Epoch 1, Batch 600, Loss: 0.167865\n",
      "Epoch 1, Batch 900, Loss: 0.074878\n",
      "Epoch 2, Batch 0, Loss: 0.054318\n",
      "Epoch 2, Batch 300, Loss: 0.039031\n",
      "Epoch 2, Batch 600, Loss: 0.017275\n",
      "Epoch 2, Batch 900, Loss: 0.161138\n",
      "Epoch 3, Batch 0, Loss: 0.072100\n",
      "Epoch 3, Batch 300, Loss: 0.118044\n",
      "Epoch 3, Batch 600, Loss: 0.013904\n",
      "Epoch 3, Batch 900, Loss: 0.012470\n",
      "Epoch 4, Batch 0, Loss: 0.043693\n",
      "Epoch 4, Batch 300, Loss: 0.075298\n",
      "Epoch 4, Batch 600, Loss: 0.019737\n",
      "Epoch 4, Batch 900, Loss: 0.040333\n",
      "Epoch 5, Batch 0, Loss: 0.044553\n",
      "Epoch 5, Batch 300, Loss: 0.100214\n",
      "Epoch 5, Batch 600, Loss: 0.194029\n",
      "Epoch 5, Batch 900, Loss: 0.013701\n",
      "Test Loss: 0.0300, Accuracy: 99.11%\n",
      "\n",
      "=== SGD z schedulером dla klasyfikacji ===\n",
      "Epoch 1, Batch 0, Loss: 2.300295\n",
      "Epoch 1, Batch 300, Loss: 0.252044\n",
      "Epoch 1, Batch 600, Loss: 0.280613\n",
      "Epoch 1, Batch 900, Loss: 0.300215\n",
      "Epoch 2, Batch 0, Loss: 0.309964\n",
      "Epoch 2, Batch 300, Loss: 0.291338\n",
      "Epoch 2, Batch 600, Loss: 0.215971\n",
      "Epoch 2, Batch 900, Loss: 0.207211\n",
      "Epoch 3, Batch 0, Loss: 0.219428\n",
      "Epoch 3, Batch 300, Loss: 0.194890\n",
      "Epoch 3, Batch 600, Loss: 0.170895\n",
      "Epoch 3, Batch 900, Loss: 0.285199\n",
      "Epoch 4, Batch 0, Loss: 0.201237\n",
      "Epoch 4, Batch 300, Loss: 0.666599\n",
      "Epoch 4, Batch 600, Loss: 0.345801\n",
      "Epoch 4, Batch 900, Loss: 0.154298\n",
      "Epoch 5, Batch 0, Loss: 0.246179\n",
      "Epoch 5, Batch 300, Loss: 0.117739\n",
      "Epoch 5, Batch 600, Loss: 0.466985\n",
      "Epoch 5, Batch 900, Loss: 0.129985\n",
      "Test Loss: 0.1116, Accuracy: 97.05%\n"
     ]
    }
   ],
   "source": [
    "# Porównanie optymizatorów na klasyfikacji\n",
    "\n",
    "# Adam\n",
    "model_adam_clf = CNNModel()\n",
    "optimizer_adam_clf = optim.Adam(model_adam_clf.parameters(), lr=0.001)\n",
    "print(\"=== Adam dla klasyfikacji ===\")\n",
    "train_classifier(model_adam_clf, optimizer_adam_clf, train_loader)\n",
    "adam_acc = test_classifier(model_adam_clf, test_loader)\n",
    "\n",
    "# AdamW\n",
    "model_adamw = CNNModel()\n",
    "optimizer_adamw = optim.AdamW(model_adamw.parameters(), lr=0.001, weight_decay=0.01)\n",
    "print(\"\\n=== AdamW dla klasyfikacji ===\")\n",
    "train_classifier(model_adamw, optimizer_adamw, train_loader)\n",
    "adamw_acc = test_classifier(model_adamw, test_loader)\n",
    "\n",
    "# SGD z schedulером\n",
    "model_sgd_clf = CNNModel()\n",
    "optimizer_sgd_clf = optim.SGD(model_sgd_clf.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_sgd_clf, step_size=3, gamma=0.1)\n",
    "print(\"\\n=== SGD z schedulером dla klasyfikacji ===\")\n",
    "train_classifier(model_sgd_clf, optimizer_sgd_clf, train_loader)\n",
    "sgd_acc = test_classifier(model_sgd_clf, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady w TensorFlow/Keras\n",
    "\n",
    "### Problem regresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Przygotowanie danych (podobnie jak w PyTorch)\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def create_regression_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, optimizer_name, optimizer, epochs=100):\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(f\"\\n=== {optimizer_name} ===\")\n",
    "    history = model.fit(X_train, y_train, \n",
    "                       batch_size=32, \n",
    "                       epochs=epochs, \n",
    "                       validation_split=0.2, \n",
    "                       verbose=0)\n",
    "    \n",
    "    final_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkorzen/PycharmProjects/pytorch_for_DL_with_python_bootcamp/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SGD ===\n",
      "Final Training Loss: nan\n",
      "Final Validation Loss: nan\n",
      "\n",
      "=== Adam ===\n",
      "Final Training Loss: 0.2256\n",
      "Final Validation Loss: 0.2844\n",
      "\n",
      "=== RMSprop ===\n",
      "Final Training Loss: 0.2339\n",
      "Final Validation Loss: 0.2834\n",
      "\n",
      "=== AdaGrad ===\n",
      "Final Training Loss: 0.2888\n",
      "Final Validation Loss: 0.3221\n"
     ]
    }
   ],
   "source": [
    "# Porównanie różnych optymizatorów w TensorFlow\n",
    "\n",
    "# SGD\n",
    "model_sgd_tf = create_regression_model()\n",
    "optimizer_sgd_tf = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "sgd_history = train_and_evaluate(model_sgd_tf, \"SGD\", optimizer_sgd_tf)\n",
    "\n",
    "# Adam\n",
    "model_adam_tf = create_regression_model()\n",
    "optimizer_adam_tf = keras.optimizers.Adam(learning_rate=0.001)\n",
    "adam_history = train_and_evaluate(model_adam_tf, \"Adam\", optimizer_adam_tf)\n",
    "\n",
    "# RMSprop\n",
    "model_rmsprop_tf = create_regression_model()\n",
    "optimizer_rmsprop_tf = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rmsprop_history = train_and_evaluate(model_rmsprop_tf, \"RMSprop\", optimizer_rmsprop_tf)\n",
    "\n",
    "# AdaGrad\n",
    "model_adagrad_tf = create_regression_model()\n",
    "optimizer_adagrad_tf = keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "adagrad_history = train_and_evaluate(model_adagrad_tf, \"AdaGrad\", optimizer_adagrad_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem klasyfikacji w TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ładowanie danych MNIST w TensorFlow\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja danych\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape dla CNN\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "def create_cnn_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_classifier_tf(model, optimizer_name, optimizer, epochs=5):\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    print(f\"\\n=== {optimizer_name} dla klasyfikacji ===\")\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       batch_size=128,\n",
    "                       epochs=epochs,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       verbose=1)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Adam dla klasyfikacji ===\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkorzen/PycharmProjects/pytorch_for_DL_with_python_bootcamp/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9289 - loss: 0.2344 - val_accuracy: 0.9849 - val_loss: 0.0491\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9756 - loss: 0.0820 - val_accuracy: 0.9879 - val_loss: 0.0370\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9808 - loss: 0.0643 - val_accuracy: 0.9877 - val_loss: 0.0371\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9851 - loss: 0.0490 - val_accuracy: 0.9897 - val_loss: 0.0329\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9866 - loss: 0.0431 - val_accuracy: 0.9914 - val_loss: 0.0289\n",
      "\n",
      "=== AdamW dla klasyfikacji ===\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.9194 - loss: 0.2606 - val_accuracy: 0.9813 - val_loss: 0.0566\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9735 - loss: 0.0889 - val_accuracy: 0.9873 - val_loss: 0.0387\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9809 - loss: 0.0646 - val_accuracy: 0.9888 - val_loss: 0.0345\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9832 - loss: 0.0563 - val_accuracy: 0.9902 - val_loss: 0.0311\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9857 - loss: 0.0465 - val_accuracy: 0.9899 - val_loss: 0.0311\n",
      "\n",
      "=== RMSprop dla klasyfikacji ===\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9275 - loss: 0.2356 - val_accuracy: 0.9853 - val_loss: 0.0463\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9759 - loss: 0.0818 - val_accuracy: 0.9869 - val_loss: 0.0359\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9813 - loss: 0.0618 - val_accuracy: 0.9888 - val_loss: 0.0313\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9849 - loss: 0.0502 - val_accuracy: 0.9903 - val_loss: 0.0288\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9865 - loss: 0.0437 - val_accuracy: 0.9904 - val_loss: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# Porównanie optymizatorów na klasyfikacji MNIST\n",
    "\n",
    "# Adam\n",
    "model_adam_clf_tf = create_cnn_model()\n",
    "optimizer_adam_clf_tf = keras.optimizers.Adam(learning_rate=0.001)\n",
    "adam_history_clf = train_classifier_tf(model_adam_clf_tf, \"Adam\", optimizer_adam_clf_tf)\n",
    "\n",
    "# AdamW (dostępny w nowszych wersjach TensorFlow)\n",
    "try:\n",
    "    model_adamw_tf = create_cnn_model()\n",
    "    optimizer_adamw_tf = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01)\n",
    "    adamw_history_clf = train_classifier_tf(model_adamw_tf, \"AdamW\", optimizer_adamw_tf)\n",
    "except AttributeError:\n",
    "    print(\"AdamW nie jest dostępny w tej wersji TensorFlow\")\n",
    "\n",
    "# RMSprop\n",
    "model_rmsprop_clf_tf = create_cnn_model()\n",
    "optimizer_rmsprop_clf_tf = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rmsprop_history_clf = train_classifier_tf(model_rmsprop_clf_tf, \"RMSprop\", optimizer_rmsprop_clf_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porównanie wyników i wnioski\n",
    "\n",
    "### Kiedy używać którego optymizatora?\n",
    "\n",
    "1. **Adam** - uniwersalny wybór dla większości problemów\n",
    "   - Szybka konwergencja\n",
    "   - Dobra performance na różnych typach danych\n",
    "   - Automatyczne dostosowywanie learning rate\n",
    "\n",
    "2. **AdamW** - dla dużych modeli wymagających regularyzacji\n",
    "   - Lepsza regularyzacja niż Adam\n",
    "   - Preferowany w Transformerach i dużych modelach\n",
    "\n",
    "3. **SGD z momentum** - dla prostych problemów i fine-tuningu\n",
    "   - Dobra generalizacja\n",
    "   - Wymaga więcej tuningu hiperparametrów\n",
    "   - Często używany z schedulerami learning rate\n",
    "\n",
    "4. **RMSprop** - dla RNN i problemów z niestacjonarnymi danymi\n",
    "   - Dobrze radzi sobie z exploding/vanishing gradients\n",
    "   - Mniej pamięci niż Adam\n",
    "\n",
    "### Praktyczne wskazówki:\n",
    "- Zacznij od **Adam** z lr=0.001\n",
    "- Jeśli model się przetrenowuje, spróbuj **AdamW**\n",
    "- Dla prostych problemów testuj **SGD z momentum**\n",
    "- Zawsze eksperymentuj z różnymi learning rates\n",
    "- Używaj schedulerów learning rate dla lepszej konwergencji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane techniki\n",
    "\n",
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Różne typy schedulerów są gotowe do użycia!\n"
     ]
    }
   ],
   "source": [
    "# PyTorch - różne typy schedulerów\n",
    "import torch.optim as optim\n",
    "\n",
    "model = RegressionModel(X_train.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# StepLR - zmniejsza lr co określoną liczbę epok\n",
    "scheduler_step = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# ExponentialLR - wykładniczo zmniejsza lr\n",
    "scheduler_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# ReduceLROnPlateau - zmniejsza lr gdy metryka przestaje się poprawiać\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "\n",
    "# CosineAnnealingLR - lr zmienia się zgodnie z funkcją cosinus\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "print(\"Różne typy schedulerów są gotowe do użycia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedulery TensorFlow są gotowe!\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow - schedulery learning rate\n",
    "import tensorflow as tf\n",
    "\n",
    "# ExponentialDecay\n",
    "lr_schedule_exp = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "# CosineDecay\n",
    "lr_schedule_cosine = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000)\n",
    "\n",
    "# PiecewiseConstantDecay\n",
    "lr_schedule_piece = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[100, 200],\n",
    "    values=[0.1, 0.02, 0.005])\n",
    "\n",
    "# Użycie z optymizatorem\n",
    "optimizer_scheduled = keras.optimizers.Adam(learning_rate=lr_schedule_exp)\n",
    "\n",
    "print(\"Schedulery TensorFlow są gotowe!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping - zapobieganie exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcja z gradient clipping jest gotowa!\n"
     ]
    }
   ],
   "source": [
    "# PyTorch - gradient clipping\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# W pętli treningowej\n",
    "def train_with_clipping(model, optimizer, data_loader, max_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 300 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "\n",
    "print(\"Funkcja z gradient clipping jest gotowa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TensorFlow z gradient clipping jest skonfigurowany!\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow - gradient clipping\n",
    "model_tf = create_cnn_model()\n",
    "\n",
    "# Optimizer z clippingiem\n",
    "optimizer_clipped = keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    clipnorm=1.0  # clipnorm lub clipvalue\n",
    ")\n",
    "\n",
    "model_tf.compile(\n",
    "    optimizer=optimizer_clipped,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model TensorFlow z gradient clipping jest skonfigurowany!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Wybór odpowiedniego optymizatora może znacząco wpłynąć na jakość i szybkość uczenia modelu. **Adam** pozostaje bezpiecznym wyborem dla większości problemów, ale warto eksperymentować z różnymi opcjami w zależności od specyfiki zadania.\n",
    "\n",
    "### Checklist wyboru optymizatora:\n",
    "1. ✅ Zacznij od Adam (lr=0.001)\n",
    "2. ✅ Jeśli model się przetrenowuje → AdamW\n",
    "3. ✅ Dla RNN → RMSprop\n",
    "4. ✅ Dla prostych problemów → SGD + momentum\n",
    "5. ✅ Zawsze używaj schedulerów learning rate\n",
    "6. ✅ Dodaj gradient clipping dla głębokich sieci\n",
    "7. ✅ Monitoruj metryki podczas treningu\n",
    "\n",
    "Pamiętaj: **nie ma jednego najlepszego optymizatora** - wybór zależy od problemu, architektury i danych!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
