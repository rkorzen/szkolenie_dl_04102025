{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570fa63c03dfebe1",
   "metadata": {},
   "source": [
    "# Gradienty w TensorFlow\n",
    "\n",
    "W tej sekcji omówimy, jak w TensorFlow działa automatyczne różniczkowanie przy użyciu `tf.GradientTape`. Skupimy się na różnicach względem PyTorch:\n",
    "* TensorFlow domyślnie działa w trybie eager (w TF 2.x), więc operacje i gradienty można śledzić „na żywo”.\n",
    "* W TensorFlow obiekty `tf.Variable` domyślnie akumulują gradienty; zwykłe tensory wymagają jawnego śledzenia.\n",
    "* Zamiast wywoływać `tensor.backward()`, używamy kontekstu `tf.GradientTape()` i metody `tape.gradient()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360b52a3dfa6b8b",
   "metadata": {},
   "source": [
    "## Propagacja wsteczna w jednym kroku\n",
    "Zaczniemy od zastosowania pojedynczej funkcji wielomianowej $y = f(x)$ do tensora $x$. Następnie wykonamy propagację wsteczną i wypiszemy gradient $\\frac {dy} {dx}$.\n",
    "\n",
    "$\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\\\\n",
    "Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$\n",
    "\n",
    "\n",
    "## Wykonaj standardowe importy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06c86ae7af4f0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:50:42.308548Z",
     "start_time": "2025-09-30T18:50:42.303820Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19a420021df6de",
   "metadata": {},
   "source": [
    "## 2. Prosty przykład z `tf.Variable`\n",
    "\n",
    "Tak jak w PyTorch używamy `requires_grad=True`, tak w TensorFlow tworzymy `tf.Variable`. Gradienty będą automatycznie śledzone dla operacji wykonanych wewnątrz `tf.GradientTape`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc171009dd3239e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:53:11.064403Z",
     "start_time": "2025-09-30T18:53:11.053872Z"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: 2*x**4 + x**3 + 3*x**2 + 5*x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9ed4b2c75f6c02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:53:12.063940Z",
     "start_time": "2025-09-30T18:53:12.040765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość funkcji y: 63.0\n",
      "Gradient dy/dx: 93.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print('Wartość funkcji y:', y.numpy())\n",
    "print('Gradient dy/dx:', dy_dx.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7b9620dc86247",
   "metadata": {},
   "source": [
    "### Uwaga\n",
    "\n",
    "Jeśli użyjemy `tf.constant` zamiast `tf.Variable`, trzeba jawnie poprosić o śledzenie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533a746f53dbbbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:53:13.198479Z",
     "start_time": "2025-09-30T18:53:13.194388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość funkcji y: 63.0\n",
      "Gradient dy/dx: 93.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # to dodajemy\n",
    "    y = f(x)\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print('Wartość funkcji y:', y.numpy())\n",
    "print('Gradient dy/dx:', dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c707183e19e0905",
   "metadata": {},
   "source": [
    "\n",
    "W PyTorch wystarczy ustawić `requires_grad=True` podczas tworzenia tensora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f58c4510add65",
   "metadata": {},
   "source": [
    "## 3. Różniczkowanie wieloetapowe\n",
    "\n",
    "Sprawdźmy działanie `GradientTape` dla dwóch warstw przekształceń. Zauważ, że `GradientTape` śledzi wszystkie operacje w swojej przestrzeni kontekstu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d9835d8b0dbb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:54:02.914351Z",
     "start_time": "2025-09-30T18:54:02.838764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient z względem x: [[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.square(x)\n",
    "    z = tf.reduce_sum(y)\n",
    "\n",
    "# Gradient z = sum(x^2) względem x to 2*x\n",
    "dz_dx = tape.gradient(z, x)\n",
    "print('Gradient z względem x:', dz_dx.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722a3d573ce38b3",
   "metadata": {},
   "source": [
    "Analogiczna operacja w PyTorch wyglądałaby następująco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7bbedb2b12243f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T18:59:14.220494Z",
     "start_time": "2025-09-30T18:59:14.013773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient z względem x: tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# zmienna wejściowa\n",
    "x = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]], requires_grad=True)\n",
    "\n",
    "# obliczenia\n",
    "y = x ** 2\n",
    "z = y.sum()\n",
    "\n",
    "# backpropagation\n",
    "z.backward()\n",
    "\n",
    "# gradient z względem x\n",
    "print(\"Gradient z względem x:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21576f4f1d89436",
   "metadata": {},
   "source": [
    "## 4. Akumulacja gradientów vs resetowanie\n",
    "\n",
    "W PyTorch gradienty w `.grad` domyślnie się kumulują, więc trzeba wywoływać `optimizer.zero_grad()`.\n",
    "\n",
    "W TensorFlow po wywołaniu `tape.gradient()` gradienty nie są przechowywane – trzeba je zastosować i ewentualnie ponownie otworzyć kontekst.\n",
    "\n",
    "TensorFlow udostępnia klasę optymalizatora, która przyjmuje gradienty wprost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dda8b8d657e4b1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:03:00.062804Z",
     "start_time": "2025-09-30T19:03:00.041102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a4252a7b000dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:03:32.490111Z",
     "start_time": "2025-09-30T19:03:32.468738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krok 1: x = 3.4000, y = 4.0000, grad = -4.0000\n",
      "Krok 2: x = 3.7200, y = 2.5600, grad = -3.2000\n",
      "Krok 3: x = 3.9760, y = 1.6384, grad = -2.5600\n",
      "Krok 4: x = 4.1808, y = 1.0486, grad = -2.0480\n",
      "Krok 5: x = 4.3446, y = 0.6711, grad = -1.6384\n"
     ]
    }
   ],
   "source": [
    "# Przykład: krótkie trenowanie pojedynczego parametru\n",
    "x = tf.Variable(3.0)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "for step in range(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(x)\n",
    "    grad = tape.gradient(y, x)\n",
    "    optimizer.apply_gradients([(grad, x)])\n",
    "    print(f'Krok {step+1}: x = {x.numpy():.4f}, y = {y.numpy():.4f}, grad = {grad.numpy():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20338661f7e388fd",
   "metadata": {},
   "source": [
    "Analogiczny kod w PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c34dfd37b0bb312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:03:25.111043Z",
     "start_time": "2025-09-30T19:03:25.068151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krok 1: x = 3.4000, y = 4.0000, grad = -4.0000\n",
      "Krok 2: x = 3.7200, y = 2.5600, grad = -3.2000\n",
      "Krok 3: x = 3.9760, y = 1.6384, grad = -2.5600\n",
      "Krok 4: x = 4.1808, y = 1.0486, grad = -2.0480\n",
      "Krok 5: x = 4.3446, y = 0.6711, grad = -1.6384\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# parametr, który będziemy trenować\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# optymalizator\n",
    "optimizer = torch.optim.SGD([x], lr=0.1)\n",
    "\n",
    "# pętla trenowania\n",
    "for step in range(5):\n",
    "    optimizer.zero_grad()     # wyczyść gradienty\n",
    "    y = f(x)                  # oblicz stratę\n",
    "    y.backward()              # policz gradient\n",
    "    optimizer.step()          # zaktualizuj parametr\n",
    "\n",
    "    print(f\"Krok {step+1}: x = {x.item():.4f}, y = {y.item():.4f}, grad = {x.grad.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f5519430543e4",
   "metadata": {},
   "source": [
    "### Różnice względem TensorFlow:\n",
    "\n",
    "* `y.backward()` ↔ `tape.gradient(...)`\n",
    "* `optimizer.step()` ↔ `optimizer.apply_gradients(...)`\n",
    "* w PyTorch trzeba **ręcznie wyzerować gradienty** (`optimizer.zero_grad()`), bo domyślnie są akumulowane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54931c44502f10",
   "metadata": {},
   "source": [
    "## Drugi przykład – suma kilku kroków\n",
    "\n",
    "Analogicznie do dynamicznego grafu w PyTorch możemy wykonywać wiele operacji, a gradienty obliczają się względem ostatniej funkcji celu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933e03c33da0c0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:23.847242Z",
     "start_time": "2025-09-30T19:20:23.827180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y3 = 25.0\n",
      "dy3/dx: [[5. 5. 5.]\n",
      " [5. 5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(tf.ones((2, 3)))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y1 = 3 * x + 2\n",
    "    y2 = tf.square(y1)\n",
    "    y3 = tf.reduce_mean(y2)\n",
    "\n",
    "dy3_dx = tape.gradient(y3, x)\n",
    "print('y3 =', y3.numpy())\n",
    "print('dy3/dx:', dy3_dx.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e000ecee50d371a",
   "metadata": {},
   "source": [
    "W PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f452005de9c3225c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:57.122806Z",
     "start_time": "2025-09-30T19:21:57.114480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y3 = 25.0\n",
      "dy3/dx: tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# zmienna wejściowa\n",
    "x = torch.ones((2, 3), requires_grad=True)\n",
    "\n",
    "# obliczenia\n",
    "y1 = 3 * x + 2\n",
    "y2 = y1 ** 2\n",
    "y3 = y2.mean()\n",
    "\n",
    "# backpropagation\n",
    "y3.backward()\n",
    "\n",
    "# gradient względem x\n",
    "print(\"y3 =\", y3.item())\n",
    "print(\"dy3/dx:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f5dd15ac574",
   "metadata": {},
   "source": [
    "🔎 Skąd ten gradient?\n",
    "\n",
    "* $ y_1 = 3x + 2 $\n",
    "* $ y_2 = (3x+2)^2 $\n",
    "* $ y_3 = \\text{mean}(y_2) $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_3}{\\partial x} = \\frac{1}{N} \\cdot 2(3x+2) \\cdot 3 = \\frac{6(3x+2)}{N}\n",
    "$$\n",
    "\n",
    "Dla\n",
    "$$\n",
    "x=1 \\rightarrow 3x+2=5; N=6\n",
    "$$\n",
    "dlatego\n",
    "$$\n",
    "gradient = \\frac{6\\cdot 5}{6} = 5\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc1d0410a14448",
   "metadata": {},
   "source": [
    "## 6. Gradienty względem wielu zmiennych\n",
    "\n",
    "Możemy przekazać listę zmiennych. TensorFlow zwróci listę gradientów w tej samej kolejności. Poniżej prosty przykład dwuparametrowej funkcji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e58c9500da9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f_val = x**2 + x*y + y**2\n",
    "\n",
    "grads = tape.gradient(f_val, [x, y])\n",
    "print('df/dx =', grads[0].numpy())\n",
    "print('df/dy =', grads[1].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b475ddcb396e3",
   "metadata": {},
   "source": [
    "## 7. Gradienty drugiego rzędu\n",
    "    Aby obliczyć pochodne wyższego rzędu, trzeba użyć `persistent=True` w `GradientTape` lub zagnieżdżonych kontekstów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dae653d31f1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2:\n",
    "    with tf.GradientTape() as tape1:\n",
    "        y = f(x)\n",
    "    dy_dx = tape1.gradient(y, x)\n",
    "d2y_dx2 = tape2.gradient(dy_dx, x)\n",
    "print('dy/dx =', dy_dx.numpy())\n",
    "print('d2y/dx2 =', d2y_dx2.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873451936d28c64",
   "metadata": {},
   "source": [
    "## 8. Porównanie z PyTorch\n",
    "\n",
    "| Aspekt | PyTorch | TensorFlow |\n",
    "| --- | --- | --- |\n",
    "| Śledzenie gradientów | `requires_grad=True` | kontekst `tf.GradientTape`, `tf.Variable` śledzi domyślnie |\n",
    "| Wyzwalanie backprop | `loss.backward()` | `tape.gradient(loss, vars)` |\n",
    "| Kumulacja gradientów | `.grad` kumuluje, trzeba zerować | gradienty zwracane jednorazowo |\n",
    "| Operacje in-place | Dozwolone (trzeba uważać) | Brak operacji in-place |\n",
    "| Domyślne obliczenia | Tryb eager | Tryb eager (TF 2.x) |\n",
    "\n",
    "    W praktyce schemat pracy jest podobny: definiujemy funkcję celu, obliczamy gradienty względem parametrów i aktualizujemy wagi przez optymalizator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754aa182d507506",
   "metadata": {},
   "source": [
    "### Świetna robota!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
